{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2FNuJP5DE0Vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Denizalp Goktas (dg2906), Da Hua Chen (dc2802)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g1Qt7TH4FMxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*In order to run the model, run all below in order*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YfqwoKIGw_tM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Up Environment"
      ]
    },
    {
      "metadata": {
        "id": "pVRz9omDjzhd",
        "colab_type": "code",
        "outputId": "df142ed6-203c-473e-f635-bcf71140a370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2068
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190227 --force-reinstall"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview==2.0.0-dev20190227\n",
            "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\",)': /packages/33/77/d923be0087f53bc864ae65334ab4ee6a97223387f1395a10400c524980bb/tf_nightly_gpu_2.0_preview-2.0.0.dev20190227-cp36-cp36m-manylinux1_x86_64.whl\u001b[0m\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/77/d923be0087f53bc864ae65334ab4ee6a97223387f1395a10400c524980bb/tf_nightly_gpu_2.0_preview-2.0.0.dev20190227-cp36-cp36m-manylinux1_x86_64.whl (331.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 331.8MB 63kB/s \n",
            "\u001b[?25hCollecting numpy<2.0,>=1.14.5 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 1.7MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.1 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/60/ca38e967360212ddbb004141a70f5f6d47296e1fba37964d8ac6cb631921/protobuf-3.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 8.3MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.3MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 25.9MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.7MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/09/3525d25dba3b8424975ecf3764aca3ac0c27b394bfcf6cc2f61727947511/tensorflow_estimator_2.0_preview-1.14.0.dev2019031300-py2.py3-none-any.whl (351kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 11.4MB/s \n",
            "\u001b[?25hCollecting gast>=0.2.0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting six>=1.10.0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting termcolor>=1.1.0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting astor>=0.6.0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
            "Collecting grpcio>=1.8.6 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/dc/5503d89e530988eb7a1aed337dcb456ef8150f7c06132233bd9e41ec0215/grpcio-1.19.0-cp36-cp36m-manylinux1_x86_64.whl (10.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 10.8MB 2.8MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "  Downloading https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/ed/81c0a17476a99b81477e3308adaa6129e055ad4a6c35b268cc8b01e54686/tb_nightly-1.14.0a20190313-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.3MB/s \n",
            "\u001b[?25hCollecting setuptools (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl (575kB)\n",
            "\u001b[K    100% |████████████████████████████████| 583kB 12.2MB/s \n",
            "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.8MB 4.3MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8 (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 6.5MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15 (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0-dev20190227)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 7.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: absl-py, gast, termcolor\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/90/db/f8/2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "Successfully built absl-py gast termcolor\n",
            "\u001b[31mjupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.15 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement six~=1.11.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, setuptools, protobuf, h5py, keras-applications, absl-py, keras-preprocessing, google-pasta, tensorflow-estimator-2.0-preview, gast, termcolor, astor, grpcio, wheel, markdown, werkzeug, tb-nightly, tf-nightly-gpu-2.0-preview\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: six 1.11.0\n",
            "    Uninstalling six-1.11.0:\n",
            "      Successfully uninstalled six-1.11.0\n",
            "  Found existing installation: setuptools 40.8.0\n",
            "    Uninstalling setuptools-40.8.0:\n",
            "      Successfully uninstalled setuptools-40.8.0\n",
            "  Found existing installation: protobuf 3.6.1\n",
            "    Uninstalling protobuf-3.6.1:\n",
            "      Successfully uninstalled protobuf-3.6.1\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Found existing installation: Keras-Applications 1.0.7\n",
            "    Uninstalling Keras-Applications-1.0.7:\n",
            "      Successfully uninstalled Keras-Applications-1.0.7\n",
            "  Found existing installation: absl-py 0.7.0\n",
            "    Uninstalling absl-py-0.7.0:\n",
            "      Successfully uninstalled absl-py-0.7.0\n",
            "  Found existing installation: Keras-Preprocessing 1.0.9\n",
            "    Uninstalling Keras-Preprocessing-1.0.9:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Found existing installation: astor 0.7.1\n",
            "    Uninstalling astor-0.7.1:\n",
            "      Successfully uninstalled astor-0.7.1\n",
            "  Found existing installation: grpcio 1.15.0\n",
            "    Uninstalling grpcio-1.15.0:\n",
            "      Successfully uninstalled grpcio-1.15.0\n",
            "  Found existing installation: wheel 0.33.1\n",
            "    Uninstalling wheel-0.33.1:\n",
            "      Successfully uninstalled wheel-0.33.1\n",
            "  Found existing installation: Markdown 3.0.1\n",
            "    Uninstalling Markdown-3.0.1:\n",
            "      Successfully uninstalled Markdown-3.0.1\n",
            "  Found existing installation: Werkzeug 0.14.1\n",
            "    Uninstalling Werkzeug-0.14.1:\n",
            "      Successfully uninstalled Werkzeug-0.14.1\n",
            "Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 google-pasta-0.1.4 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.0.1 numpy-1.16.2 protobuf-3.7.0 setuptools-40.8.0 six-1.12.0 tb-nightly-1.14.0a20190313 tensorflow-estimator-2.0-preview-1.14.0.dev2019031300 termcolor-1.1.0 tf-nightly-gpu-2.0-preview-2.0.0.dev20190227 werkzeug-0.14.1 wheel-0.33.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "grpc",
                  "numpy",
                  "pkg_resources",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NmM3s2MovBkl",
        "colab_type": "code",
        "outputId": "3ac8aa3c-8e53-4bff-971b-6237e653712d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B2X3zXZUw5uo",
        "colab_type": "code",
        "outputId": "047085df-72fb-48f3-abc9-148c9b6875d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os, sys, time \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "path = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/\"\n",
        "os.chdir(path) \n",
        "!python -c 'import keras; print(keras.__version__)'\n",
        "!python3 -c 'import tensorflow as tf; print(tf.__version__)'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2.2.4\n",
            "2.0.0-dev20190227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82uIYb1mfAw4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Modified Unet blocks"
      ]
    },
    {
      "metadata": {
        "id": "Bqw6tPpHfIcq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The models below are a Unet model modified so that they have the ResNet \"property\" which allows the gradients to not vanish and improves training. We achieve this by putting skip connections in between the convolutional layers."
      ]
    },
    {
      "metadata": {
        "id": "E1diJa5qfARL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Modify Unet with skips to implement resnet\n",
        "\n",
        "def conv_block(x, n_channels, droprate = 0.25):\n",
        "    \"\"\" for UNet \"\"\"\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x_short = x\n",
        "    x = Conv1D(n_channels, 3, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    x = Dropout(droprate)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_short])\n",
        "    x = ReLU()(x)\n",
        "    x_short = x\n",
        "    x = Conv1D(n_channels, 3, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_short])\n",
        "    return x\n",
        "\n",
        "def up_block(x, n_channels):\n",
        "    \"\"\" for UNet \"\"\"\n",
        "    x_short = x\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = UpSampling1D(size = 2)(x)\n",
        "    x = Conv1D(n_channels, 2, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "    x = Add()([x, x_short])\n",
        "    return x\n",
        "\n",
        "def Conv_UNet(x, droprate=0.25):\n",
        "    \"\"\" 1-D Convolutional UNet https://arxiv.org/abs/1505.04597 \"\"\"\n",
        "\n",
        "    conv0 = Conv1D(192, 3, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
        "\n",
        "    conv1 = conv_block(conv0, 128, droprate)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, 192, droprate)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, 384, droprate)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
        "\n",
        "    conv4 = conv_block(pool3, 512, droprate)\n",
        "\n",
        "    pool4 = MaxPooling1D(pool_size=2)(conv4)\n",
        "    conv5 = conv_block(pool4, 1024, droprate)\n",
        "    up5 = conv5\n",
        "\n",
        "    up4 = up_block(up5, 512)\n",
        "    up4 = concatenate([conv4,up4], axis = 2)\n",
        "    up4 = conv_block(up4, 512, droprate)\n",
        "\n",
        "    up4 = conv4\n",
        "\n",
        "    up3 = up_block(up4, 384)\n",
        "    up3 = concatenate([conv3,up3], axis = 2)\n",
        "    up3 = conv_block(up3, 384, droprate)\n",
        "\n",
        "    up2 = up_block(up3, 192)\n",
        "    up2 = concatenate([conv2,up2], axis = 2)\n",
        "    up2 = conv_block(up2, 192, droprate)\n",
        "\n",
        "    up1 = up_block(up2, 128)\n",
        "    up1 = concatenate([conv1,up1], axis = 2)\n",
        "    up1 = conv_block(up1, 128, droprate)\n",
        "\n",
        "    up1 = BatchNormalization()(up1)\n",
        "    up1 = ReLU()(up1)\n",
        "\n",
        "    return up1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xaziNhWEgT8m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unet + RNN model with attention"
      ]
    },
    {
      "metadata": {
        "id": "TLlEocwFIAnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we have implemented only one model that outputs distances and angles by using the MSA features since it outputted the best result for our purposes. The model below is our submission for all three questions"
      ]
    },
    {
      "metadata": {
        "id": "0RMvubPHFyMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The model is composed of the following:**\n",
        "\n",
        "\n",
        "1.   Embed raw sequences and the q8 sequences, pad all entries so that they are of the same size. \n",
        "\n",
        "2.   Concatenate the embedded sequences and MSA informations.\n",
        "\n",
        "3.  Take the modified Conv_unet of the concatenated input.\n",
        "\n",
        "4. Multiply each entry of the Conv_unet output by the outputs of the attention mechanism implemented for the Conv_unet.\n",
        "\n",
        "5. Take the Bidirectional GRU of the initial concatenated input.\n",
        "\n",
        "6. Multiply each entry of the Bidirectional GRU output by the outputs of the attention mechanism implemented for the GRU.\n",
        "\n",
        "7. Concatenate the the GRU with attention and Conv_unet layers together.\n",
        "\n",
        "8. Make the concatenated output in step 7 go through a dense layer with softmax activation\n",
        "\n",
        "9. Use the output to predict angles and distance matrices.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZQy8IJzlIR6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters**: \n",
        "1. Epochs : 60\n",
        "2. Piecewise constant decay optimizer with decays steps of [0.0001, 0.00005, 0.00002]  that changed at the 30th and the 48th iterations."
      ]
    },
    {
      "metadata": {
        "id": "TbOFF7kP6v_U",
        "colab_type": "code",
        "outputId": "ddeeab6f-e787-4663-a564-1f0cfeb3c813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21346
        }
      },
      "cell_type": "code",
      "source": [
        "import os, sys, time \n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.layers import *\n",
        "from model_utils import *\n",
        "from datetime import datetime\n",
        "\n",
        "#######################################\n",
        "# require tensorflow 2.0 \n",
        "# CPU-only: pip install tf-nightly-2.0-preview\n",
        "# GPU: pip install tf-nightly-2.0-gpu-preview\n",
        "print(tf.__version__)\n",
        "#######################################\n",
        "\n",
        "# python train_unet.py\n",
        "\n",
        "model_name = os.path.basename(sys.argv[0]).split(\".\")[0]   # or model name\n",
        "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-\" + model_name\n",
        "\n",
        "#######################################\n",
        "# Load data \n",
        "#######################################\n",
        "\n",
        "# replace this with your data directory\n",
        "data_dir = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/cu-deep-learning-spring19-hw2/\"\n",
        "\n",
        "folds_use = [1,2,3,4,5,6,7,8,9,10]\n",
        "data_fields = [[] for _ in range(9)]\n",
        "for i in folds_use:\n",
        "    with open(os.path.join(data_dir, 'train_fold_{}.pkl'.format(i)), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        for j in range(len(data_fields)):\n",
        "            data_fields[j].append(data[j])\n",
        "\n",
        "for j in range(len(data_fields)):\n",
        "    data_fields[j] = np.concatenate(data_fields[j])\n",
        "indices, pdbs, length_aas, pdb_aas, q8s, dcalphas, psis, phis, msas = data_fields\n",
        "print(\"Total number of protein sequences:\", len(pdb_aas))\n",
        "\n",
        "# filter out sequences that are too long \n",
        "maxlen_seq = 384\n",
        "minlen_seq = 10\n",
        "lenth_mask = (length_aas < maxlen_seq) & (length_aas > minlen_seq)\n",
        "for j in range(len(data_fields)):\n",
        "    data_fields[j] = data_fields[j][lenth_mask]\n",
        "indices, pdbs, length_aas, pdb_aas, q8s, dcalphas, psis, phis, msas = data_fields\n",
        "msas = [np.stack(x).transpose().astype(np.float32) for x in msas]\n",
        "phis = [np.array(x) for x in phis]\n",
        "psis = [np.array(x) for x in psis]\n",
        "print(\"Number of protein sequences shorter than {}: {}\".format(maxlen_seq, len(pdb_aas)))\n",
        "\n",
        "#############################################\n",
        "# Prepare dataset for traininng \n",
        "#############################################\n",
        "\n",
        "# Pad target distance matrix to the same size \n",
        "# Mask the padded regions later so that they don't contribute to the loss\n",
        "dcalphas_pad = np.zeros((len(dcalphas), maxlen_seq, maxlen_seq), dtype=np.float32)\n",
        "for i in range(len(dcalphas)):\n",
        "    length = dcalphas[i].shape[0]\n",
        "    dcalphas_pad[i, :length, :length] = dcalphas[i]\n",
        "\n",
        "# Pad target torsion angles to the same size \n",
        "train_target_phis = np.zeros([len(phis), maxlen_seq], dtype=np.float32)\n",
        "for i in range(len(phis)):\n",
        "    train_target_phis[i, :phis[i].shape[0]] = phis[i]\n",
        "train_target_psis = np.zeros([len(psis), maxlen_seq], dtype=np.float32)\n",
        "for i in range(len(psis)):\n",
        "    train_target_psis[i, :psis[i].shape[0]] = psis[i]\n",
        "\n",
        "# Input data \n",
        "train_input_seqs = pdb_aas\n",
        "train_input_grams = seq2ngrams(train_input_seqs, n=1)\n",
        "train_q8s = q8s\n",
        "train_q8s_grams = seq2ngrams(train_q8s, n=1)\n",
        "\n",
        "# Define tokenizer encoder the input sequence\n",
        "tokenizer_encoder = text.Tokenizer()\n",
        "tokenizer_encoder.fit_on_texts(train_input_grams)\n",
        "tokenizer_encoder_q8s = text.Tokenizer()\n",
        "tokenizer_encoder_q8s.fit_on_texts(train_q8s_grams)\n",
        "\n",
        "# Tokenize the input sequences for use in training\n",
        "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams)\n",
        "train_input_data = sequence.pad_sequences(train_input_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "train_q8s_data = tokenizer_encoder_q8s.texts_to_sequences(train_q8s_grams)\n",
        "train_q8s_data = sequence.pad_sequences(train_q8s_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "\n",
        "# The number of words and tags to be passed as parameters to the model\n",
        "n_words = len(tokenizer_encoder.word_index) + 1\n",
        "n_words_q8s = len(tokenizer_encoder_q8s.word_index) + 1\n",
        "\n",
        "# extra MSA features \n",
        "msa_dim = msas[0].shape[1]\n",
        "msas_padded = np.zeros([len(msas), maxlen_seq, msa_dim], dtype=np.float32)\n",
        "for i in range(len(msas)):\n",
        "    msas_padded[i, :msas[i].shape[0], :] = msas[i]\n",
        "\n",
        "# Train/validation set split \n",
        "training_idx = np.arange(2800)\n",
        "validation_idx = np.arange(2800, len(pdb_aas))\n",
        "\n",
        "X_train = train_input_data[training_idx]\n",
        "X_val = train_input_data[validation_idx]\n",
        "\n",
        "X_train_q8s = train_q8s_data[training_idx]\n",
        "X_val_q8s = train_q8s_data[validation_idx]\n",
        "\n",
        "X_train_msa = msas_padded[training_idx]\n",
        "X_val_msa = msas_padded[validation_idx]\n",
        "\n",
        "X_train_seqlen = length_aas[training_idx]\n",
        "X_val_seqlen = length_aas[validation_idx]\n",
        "\n",
        "y_train_dist_matrix = dcalphas_pad[training_idx]\n",
        "y_val_dist_matrix = dcalphas_pad[validation_idx]\n",
        "\n",
        "y_train_phis = train_target_phis[training_idx]\n",
        "y_val_phis = train_target_phis[validation_idx]\n",
        "y_train_psis = train_target_psis[training_idx]\n",
        "y_val_psis = train_target_psis[validation_idx]\n",
        "\n",
        "pdb_names_train = pdbs[training_idx]\n",
        "pdb_names_val = pdbs[validation_idx]\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Create a directory to save model checkpoints \n",
        "#############################################\n",
        "\n",
        "if not os.path.exists(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "log_dir = 'logs/{}'.format(model_name)\n",
        "os.mkdir(log_dir)\n",
        "\n",
        "with open(os.path.join(log_dir, 'tokenizer_encoder.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer_encoder, handle)\n",
        "\n",
        "with open(os.path.join(log_dir, 'tokenizer_encoder_q8s.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer_encoder_q8s, handle)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# build a neural network model\n",
        "#############################################\n",
        "\n",
        "# Here we use layer functions to build neural network graph (as opposed to defining a Model class)\n",
        "\n",
        "# Input layer: (None, ) shape allows the network to be flexible to input sequence length \n",
        "# inputs: [protein_sequence, q8_sequence, MSA features]\n",
        "inputs = [Input(shape = (None, )), Input(shape = (None, )), Input(shape = (None, 21))]\n",
        "\n",
        "# Word (amino acid) embedding layers \n",
        "embedding1 = Embedding(input_dim = n_words, output_dim = 128, input_length = None, input_shape=(None,))\n",
        "embedding2 = Embedding(input_dim = n_words_q8s, output_dim = 64, input_length = None, input_shape=(None,))\n",
        "embed_input = embedding1(inputs[0]) # raw seq\n",
        "embed_q8s = embedding2(inputs[1]) # q8\n",
        "\n",
        "\n",
        "merged_input = concatenate([embed_input, embed_q8s, inputs[2]], axis = 2)\n",
        "\n",
        "######### Raw Sequence data\n",
        "\n",
        "#### A seq-to-seq 1-D convolutional U-Net (https://arxiv.org/abs/1505.04597)\n",
        "# raw_seq = Conv_UNet(embed_input, droprate=0.1)\n",
        "# q8_seq =  Conv_UNet(embed_q8s, droprate=0.1)\n",
        "# msa_seq = Conv_UNet(inputs[2], droprate=0.1)\n",
        "conv1 = Conv_UNet(merged_input, droprate = 0.3)\n",
        "\n",
        "#### bidirectional RNN\n",
        "rnn = Bidirectional(GRU(units=32, return_sequences=True, recurrent_dropout=0.2, recurrent_activation='relu', bias_initializer= \"ones\"))\n",
        "GRU1 = rnn(merged_input)\n",
        "GRU1 = BatchNormalization()(conv1)\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Attention\n",
        "##################################################\n",
        "\n",
        "attention1 = Dense(1, activation='tanh')(GRU1)\n",
        "attention1 = Flatten()(attention1)\n",
        "attention1 = Activation('softmax')(attention1)\n",
        "attention1 = RepeatVector(rnn.output_shape[2]*2)(attention1)\n",
        "attention1 = Permute([2, 1])(attention1)\n",
        "\n",
        "\n",
        "attention_merge1 = multiply([attention1, GRU1])\n",
        "\n",
        "\n",
        "\n",
        "attention2 = Dense(1, activation='tanh')(conv1)\n",
        "attention2 = Flatten()(attention2)\n",
        "attention2 = Activation('softmax')(attention2)\n",
        "attention2 = RepeatVector(conv1.shape[2])(attention2)\n",
        "attention2 = Permute([2, 1])(attention2)\n",
        "\n",
        "\n",
        "attention_merge2 = multiply([attention2, conv1])\n",
        "\n",
        "#### A seq-to-seq 1-D convolutional U-Net (https://arxiv.org/abs/1505.04597)\n",
        "\n",
        "result = concatenate([attention_merge1, attention_merge2])\n",
        "\n",
        "#### Torsion angle prediction\n",
        "# The model output 3 angles (phi, psi, omega) for computing distance matrix. \n",
        "# But we only use phi, psi for evaluating angles since omega angle is usually fixed in the protein. \n",
        "#y = TimeDistributed(Dense(3, activation = \"tanh\"))(merged_out)\n",
        "#angles = tf.multiply(y, np.pi, name=\"torsion_angles\") \n",
        "\n",
        "# The RGN paper predict torsion angles using softmax probabilities over a learned alphabet/mixture of angles (https://www.biorxiv.org/content/biorxiv/early/2018/08/29/265231.full-text.pdf)\n",
        "y = TimeDistributed(Dense(50, activation = \"softmax\"))(result)\n",
        "angles = TorsionAngles(alphabet_size=50)(y)\n",
        "\n",
        "#### Build Model. \n",
        "# Define the computational graph using model input and model output\n",
        "model = tf.keras.Model(inputs, [angles, y])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Train the model\n",
        "##################################################\n",
        "\n",
        "batch_size = 32\n",
        "angle_scale = 180 / np.pi  # convert angles from [-pi, pi] to [-180, 180]\n",
        "n_iter = int(X_train.shape[0] / batch_size)\n",
        "\n",
        "#### For RNN (slower)\n",
        "# n_epochs = 8\n",
        "# lr_decay_iters = [n_iter * 3.0, n_iter * 6.0]  # lr decay at n_iters * epoch_steps\n",
        "# lr_steps = [0.001, 0.0005, 0.0001]  # learning rate decay steps\n",
        "\n",
        "#### For ConvNet\n",
        "n_epochs = 60\n",
        "lr_decay_iters = [n_iter * 5.0, n_iter * 8.0]  # lr decay at n_iters * epoch_steps\n",
        "lr_steps = [0.0001, 0.00005, 0.00002]  # learning rate decay steps\n",
        "\n",
        "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_decay_iters, lr_steps)\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate_fn)\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "torsion_loss_weight = 1.0 / 50\n",
        "dist_loss_weight = 1.0 \n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    \n",
        "    ################################### \n",
        "    # Train\n",
        "    ################################### \n",
        "    idx_shuffle = np.random.permutation(X_train.shape[0])\n",
        "\n",
        "    train_rmsd_dist = tf.metrics.Mean()\n",
        "    train_rmsd_dist_norm = tf.metrics.Mean()\n",
        "    train_rmsd_angle = tf.metrics.Mean()\n",
        "    train_rmsd_all = tf.metrics.Mean()\n",
        "\n",
        "    # Iterate through mini-batchs\n",
        "    for it in range(n_iter):\n",
        "        # Shuffle (alternatively, use tf.dataset)\n",
        "        idx_batch = idx_shuffle[it * batch_size : (it+1) * batch_size]\n",
        "        batch_seq = tf.convert_to_tensor(X_train[idx_batch])\n",
        "        batch_q8s = tf.convert_to_tensor(X_train_q8s[idx_batch])\n",
        "        batch_seqlen = tf.convert_to_tensor(X_train_seqlen[idx_batch], dtype=tf.float32)\n",
        "        batch_dcalphas = tf.convert_to_tensor(y_train_dist_matrix[idx_batch])\n",
        "        batch_msa = tf.convert_to_tensor(X_train_msa[idx_batch])\n",
        "\n",
        "        # Compute loss \n",
        "        with tf.GradientTape() as tape:\n",
        "            torsion_angles, y_out = model([batch_seq, batch_q8s, batch_msa])\n",
        "            phi, psi = torsion_angles[:, :, 0], torsion_angles[:, :, 1]  \n",
        "            phi_scaled, psi_scaled = phi * angle_scale, psi * angle_scale  # from [-pi, pi] to [-180, 180]\n",
        "            loss_phi_batch = rmsd_torsion_angle(phi_scaled, y_train_phis[idx_batch], batch_seqlen)\n",
        "            loss_psi_batch = rmsd_torsion_angle(psi_scaled, y_train_psis[idx_batch], batch_seqlen)\n",
        "            loss_phi = tf.reduce_mean(loss_phi_batch)  \n",
        "            loss_psi = tf.reduce_mean(loss_psi_batch)\n",
        "\n",
        "            dist_matrix, coordinates = DistanceMatrix()(torsion_angles)\n",
        "            loss_drmsd_batch = drmsd_dist_matrix(dist_matrix, batch_dcalphas, batch_seqlen)\n",
        "            loss_drmsd = tf.reduce_mean(loss_drmsd_batch)  # drmsd metric\n",
        "            loss_drmsd_normalized = tf.reduce_mean(loss_drmsd_batch / tf.sqrt(batch_seqlen))  # longer proteins have larger distance\n",
        "            \n",
        "            # the optimization objective can be 1. loss_drmsd or 2. (loss_phi + loss_psi), or both\n",
        "            # optimizing distance mattrix (drmsd) and tortion angles separately probably gives better rerults. \n",
        "            # loss_all = loss_drmsd   # distance matrix loss\n",
        "            # loss_all = loss_phi + loss_psi  # torsion angle loss\n",
        "            \n",
        "            loss_all = loss_drmsd * dist_loss_weight + (loss_phi + loss_psi) * torsion_loss_weight\n",
        "            \n",
        "\n",
        "        # Compute gradient \n",
        "        grads = tape.gradient(loss_all, model.trainable_variables)  # loss includ both distance matrix and torsion angles \n",
        "        # grads = tape.gradient(loss_all, model.trainable_variables)\n",
        "        grads, global_norm = tf.clip_by_global_norm(grads, 5.0)\n",
        "        \n",
        "        # Backprop\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\n",
        "\n",
        "        # Record metrics\n",
        "        train_rmsd_dist_norm(loss_drmsd_normalized)\n",
        "        train_rmsd_dist(loss_drmsd)\n",
        "        train_rmsd_angle((loss_phi + loss_psi)/2)\n",
        "        train_rmsd_all(loss_all)\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            print(\"Epoch {:04d} Batch {:03d}/{:03d}: Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g}\".format(\n",
        "                epoch, it, n_iter, loss_all, loss_drmsd, loss_drmsd_normalized, (loss_phi + loss_psi)/2))\n",
        "        \n",
        "    \n",
        "    ###################################  \n",
        "    # Validation \n",
        "    ################################### \n",
        "    idx_val = np.arange(X_val.shape[0])\n",
        "    n_iter_val = int(len(idx_val) / batch_size) + 1  # use all\n",
        "\n",
        "    val_rmsd_dist = tf.metrics.Mean()\n",
        "    val_rmsd_dist_norm = tf.metrics.Mean()\n",
        "    val_rmsd_angle = tf.metrics.Mean()\n",
        "    val_rmsd_all = tf.metrics.Mean()\n",
        "\n",
        "    for it in range(n_iter_val):\n",
        "        idx_batch = idx_val[it * batch_size : (it+1) * batch_size]\n",
        "        batch_seq = tf.convert_to_tensor(X_val[idx_batch])\n",
        "        batch_q8s = tf.convert_to_tensor(X_val_q8s[idx_batch])\n",
        "        batch_seqlen = tf.convert_to_tensor(X_val_seqlen[idx_batch], dtype=tf.float32)\n",
        "        batch_dcalphas = tf.convert_to_tensor(y_val_dist_matrix[idx_batch])\n",
        "        batch_msa = tf.convert_to_tensor(X_val_msa[idx_batch])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            torsion_angles, y_out = model([batch_seq, batch_q8s, batch_msa])\n",
        "            phi, psi = torsion_angles[:, :, 0], torsion_angles[:, :, 1]\n",
        "            phi_scaled, psi_scaled = phi * angle_scale, psi * angle_scale  # from [-pi, pi] to [-180, 180]\n",
        "            loss_phi_batch = rmsd_torsion_angle(phi_scaled, y_val_phis[idx_batch], batch_seqlen)\n",
        "            loss_psi_batch = rmsd_torsion_angle(psi_scaled, y_val_psis[idx_batch], batch_seqlen)\n",
        "            loss_phi = tf.reduce_mean(loss_phi_batch)  \n",
        "            loss_psi = tf.reduce_mean(loss_psi_batch)\n",
        "\n",
        "            # torsion_angles are on the scale of [-3.14, 3.14], the target labels are on the scale of [-180, 180]. Need to convert after prediction\n",
        "            dist_matrix, coordinates = DistanceMatrix()(torsion_angles)\n",
        "            loss_drmsd_batch = drmsd_dist_matrix(dist_matrix, batch_dcalphas, batch_seqlen)\n",
        "            loss_drmsd = tf.reduce_mean(loss_drmsd_batch)\n",
        "            loss_drmsd_normalized = tf.reduce_mean(loss_drmsd_batch / tf.sqrt(batch_seqlen))\n",
        "\n",
        "            #loss_all = loss_drmsd\n",
        "            # loss_all = loss_phi + loss_psi\n",
        "            loss_all = loss_drmsd * dist_loss_weight + (loss_phi + loss_psi) * torsion_loss_weight\n",
        "\n",
        "        # no gradient descent during validation\n",
        "        val_rmsd_dist_norm(loss_drmsd_normalized)\n",
        "        val_rmsd_dist(loss_drmsd)\n",
        "        val_rmsd_angle((loss_phi + loss_psi)/2)\n",
        "        val_rmsd_all(loss_all)\n",
        "\n",
        "        if it == 0:\n",
        "            plot_pred = dist_matrix[:8].numpy()\n",
        "            plot_gt = batch_dcalphas[:8].numpy()\n",
        "            plot_names = pdb_names_val[idx_batch][:8]\n",
        "            plot_len = batch_seqlen[:8].numpy().astype(int)\n",
        "            plot_rmds = loss_drmsd_batch[:8].numpy()\n",
        "            plot_dist_matrix(plot_pred, plot_gt, plot_names, plot_len, plot_rmds,\n",
        "                os.path.join(log_dir, \"plot_distance_matrix_epoch-{}.pdf\".format(epoch)))\n",
        "\n",
        "    ################################### \n",
        "\n",
        "    # plot history if not using tensorboard\n",
        "    train_loss_history.append([train_rmsd_all.result(), train_rmsd_dist.result(), train_rmsd_dist_norm.result(), train_rmsd_angle.result()])\n",
        "    val_loss_history.append([val_rmsd_all.result(), val_rmsd_dist.result(), val_rmsd_dist_norm.result(), val_rmsd_angle.result()])\n",
        "\n",
        "    print(\"\\n===========================================\\n\")\n",
        "    print(\"Epoch {:03d}: Train Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g} \\n \".format(\n",
        "        epoch, *train_loss_history[-1]) + \n",
        "        \"             Val Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g}\".format(\n",
        "        *val_loss_history[-1]))\n",
        "    print(\"\\n===========================================\\n\")\n",
        "\n",
        "    # save model checkpoints (this overwrites previous checkpoints)\n",
        "    tf.saved_model.save(model, log_dir)\n",
        "\n",
        "    # plot\n",
        "    train_loss_arr = np.array(train_loss_history).transpose()\n",
        "    val_loss_arr = np.array(val_loss_history).transpose()\n",
        "    plot_train_val(train_loss_arr[0], val_loss_arr[0], \n",
        "            title=\"RMSD (dist + angle) loss\", savepath=os.path.join(log_dir, \"rmsd_all.pdf\"))\n",
        "    plot_train_val(train_loss_arr[1], val_loss_arr[1], \n",
        "            title=\"RMSD (distance matrix) loss\", savepath=os.path.join(log_dir, \"rmsd_distance_matrix.pdf\"))\n",
        "    plot_train_val(train_loss_arr[2], val_loss_arr[2], \n",
        "            title=\"RMSD (distance matrix, lengh-normalized) loss\", savepath=os.path.join(log_dir, \"rmsd_distance_matrix_normalized.pdf\"))\n",
        "    plot_train_val(train_loss_arr[3], val_loss_arr[3], \n",
        "            title=\"RMSD (torsion angles) loss\", savepath=os.path.join(log_dir, \"rmsd_torsion_angles.pdf\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-dev20190227\n",
            "Total number of protein sequences: 3449\n",
            "Number of protein sequences shorter than 384: 3155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0312 22:18:14.363067 140693574170496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4080: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 128)    2688        input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 64)     576         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, None, 21)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, None, 213)    0           embedding_4[0][0]                \n",
            "                                                                 embedding_5[0][0]                \n",
            "                                                                 input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_46 (Conv1D)              (None, None, 192)    122880      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_46 (Batc (None, None, 192)    768         conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_46 (ReLU)                 (None, None, 192)    0           batch_normalization_v2_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_47 (Conv1D)              (None, None, 128)    73856       re_lu_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, None, 128)    0           conv1d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_47 (Batc (None, None, 128)    512         dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_47 (ReLU)                 (None, None, 128)    0           batch_normalization_v2_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_48 (Conv1D)              (None, None, 128)    49280       re_lu_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, None, 128)    0           conv1d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_48 (Batc (None, None, 128)    512         max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_48 (ReLU)                 (None, None, 128)    0           batch_normalization_v2_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_49 (Conv1D)              (None, None, 192)    73920       re_lu_48[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, None, 192)    0           conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_49 (Batc (None, None, 192)    768         dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_49 (ReLU)                 (None, None, 192)    0           batch_normalization_v2_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_50 (Conv1D)              (None, None, 192)    110784      re_lu_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, None, 192)    0           conv1d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_50 (Batc (None, None, 192)    768         max_pooling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_50 (ReLU)                 (None, None, 192)    0           batch_normalization_v2_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_51 (Conv1D)              (None, None, 384)    221568      re_lu_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, None, 384)    0           conv1d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_51 (Batc (None, None, 384)    1536        dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_51 (ReLU)                 (None, None, 384)    0           batch_normalization_v2_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_52 (Conv1D)              (None, None, 384)    442752      re_lu_51[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, None, 384)    0           conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_52 (Batc (None, None, 384)    1536        max_pooling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_52 (ReLU)                 (None, None, 384)    0           batch_normalization_v2_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_53 (Conv1D)              (None, None, 512)    590336      re_lu_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, None, 512)    0           conv1d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_53 (Batc (None, None, 512)    2048        dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_53 (ReLU)                 (None, None, 512)    0           batch_normalization_v2_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_54 (Conv1D)              (None, None, 512)    786944      re_lu_53[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_59 (Batc (None, None, 512)    2048        conv1d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_59 (ReLU)                 (None, None, 512)    0           batch_normalization_v2_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling1d_9 (UpSampling1D)  (None, None, 512)    0           re_lu_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_60 (Conv1D)              (None, None, 384)    393600      up_sampling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, None, 768)    0           conv1d_52[0][0]                  \n",
            "                                                                 conv1d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_60 (Batc (None, None, 768)    3072        concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_60 (ReLU)                 (None, None, 768)    0           batch_normalization_v2_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_61 (Conv1D)              (None, None, 384)    885120      re_lu_60[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, None, 384)    0           conv1d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_61 (Batc (None, None, 384)    1536        dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_61 (ReLU)                 (None, None, 384)    0           batch_normalization_v2_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_62 (Conv1D)              (None, None, 384)    442752      re_lu_61[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_62 (Batc (None, None, 384)    1536        conv1d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_62 (ReLU)                 (None, None, 384)    0           batch_normalization_v2_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling1d_10 (UpSampling1D) (None, None, 384)    0           re_lu_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_63 (Conv1D)              (None, None, 192)    147648      up_sampling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, None, 384)    0           conv1d_50[0][0]                  \n",
            "                                                                 conv1d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_63 (Batc (None, None, 384)    1536        concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_63 (ReLU)                 (None, None, 384)    0           batch_normalization_v2_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_64 (Conv1D)              (None, None, 192)    221376      re_lu_63[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, None, 192)    0           conv1d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_64 (Batc (None, None, 192)    768         dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_64 (ReLU)                 (None, None, 192)    0           batch_normalization_v2_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_65 (Conv1D)              (None, None, 192)    110784      re_lu_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_65 (Batc (None, None, 192)    768         conv1d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_65 (ReLU)                 (None, None, 192)    0           batch_normalization_v2_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling1d_11 (UpSampling1D) (None, None, 192)    0           re_lu_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_66 (Conv1D)              (None, None, 128)    49280       up_sampling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, None, 256)    0           conv1d_48[0][0]                  \n",
            "                                                                 conv1d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_66 (Batc (None, None, 256)    1024        concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_66 (ReLU)                 (None, None, 256)    0           batch_normalization_v2_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_67 (Conv1D)              (None, None, 128)    98432       re_lu_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, None, 128)    0           conv1d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_67 (Batc (None, None, 128)    512         dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_67 (ReLU)                 (None, None, 128)    0           batch_normalization_v2_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_68 (Conv1D)              (None, None, 128)    49280       re_lu_67[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_68 (Batc (None, None, 128)    512         conv1d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_68 (ReLU)                 (None, None, 128)    0           batch_normalization_v2_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v2_69 (Batc (None, None, 128)    512         re_lu_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, None, 1)      129         batch_normalization_v2_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, None, 1)      129         re_lu_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, None)         0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, None)         0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None)         0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None)         0           flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 128, None)    0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 128, None)    0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, None, 128)    0           repeat_vector_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, None, 128)    0           repeat_vector_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, None, 128)    0           permute_2[0][0]                  \n",
            "                                                                 batch_normalization_v2_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, None, 128)    0           permute_3[0][0]                  \n",
            "                                                                 re_lu_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, None, 256)    0           multiply_2[0][0]                 \n",
            "                                                                 multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 50)     12850       concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "torsion_angles_2 (TorsionAngles (None, None, 3)      150         time_distributed_2[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 4,909,386\n",
            "Trainable params: 4,898,250\n",
            "Non-trainable params: 11,136\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 0001 Batch 000/087: Loss: 197.13, dist RMSD: 188.15, (length-normalized): 14.39, angle RMSD: 224.59\n",
            "Epoch 0001 Batch 010/087: Loss: 130.97, dist RMSD: 123.62, (length-normalized): 9.6195, angle RMSD: 183.75\n",
            "Epoch 0001 Batch 020/087: Loss: 32.352, dist RMSD: 25.037, (length-normalized): 1.9583, angle RMSD: 182.88\n",
            "Epoch 0001 Batch 030/087: Loss: 28.343, dist RMSD: 20.955, (length-normalized): 1.6122, angle RMSD: 184.69\n",
            "Epoch 0001 Batch 040/087: Loss: 22.519, dist RMSD: 15.447, (length-normalized): 1.4002, angle RMSD: 176.8\n",
            "Epoch 0001 Batch 050/087: Loss: 23.74, dist RMSD: 16.956, (length-normalized): 1.5303, angle RMSD: 169.6\n",
            "Epoch 0001 Batch 060/087: Loss: 29.006, dist RMSD: 22.009, (length-normalized): 1.9013, angle RMSD: 174.94\n",
            "Epoch 0001 Batch 070/087: Loss: 26.215, dist RMSD: 19.244, (length-normalized): 1.7837, angle RMSD: 174.28\n",
            "Epoch 0001 Batch 080/087: Loss: 40.048, dist RMSD: 33.295, (length-normalized): 2.6766, angle RMSD: 168.82\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 001: Train Loss: 48.475, dist RMSD: 41.295, (length-normalized): 3.3213, angle RMSD: 179.48 \n",
            "              Val Loss: 25.17, dist RMSD: 18.475, (length-normalized): 1.6891, angle RMSD: 167.37\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0002 Batch 000/087: Loss: 24.762, dist RMSD: 18.086, (length-normalized): 1.7531, angle RMSD: 166.9\n",
            "Epoch 0002 Batch 010/087: Loss: 22.705, dist RMSD: 16.107, (length-normalized): 1.4434, angle RMSD: 164.96\n",
            "Epoch 0002 Batch 020/087: Loss: 24.78, dist RMSD: 17.465, (length-normalized): 1.4732, angle RMSD: 182.86\n",
            "Epoch 0002 Batch 030/087: Loss: 23.113, dist RMSD: 16.317, (length-normalized): 1.5406, angle RMSD: 169.9\n",
            "Epoch 0002 Batch 040/087: Loss: 20.697, dist RMSD: 14.064, (length-normalized): 1.3159, angle RMSD: 165.83\n",
            "Epoch 0002 Batch 050/087: Loss: 21.994, dist RMSD: 14.66, (length-normalized): 1.2694, angle RMSD: 183.35\n",
            "Epoch 0002 Batch 060/087: Loss: 26.303, dist RMSD: 19.579, (length-normalized): 1.6791, angle RMSD: 168.11\n",
            "Epoch 0002 Batch 070/087: Loss: 22.877, dist RMSD: 16.311, (length-normalized): 1.4662, angle RMSD: 164.17\n",
            "Epoch 0002 Batch 080/087: Loss: 19.953, dist RMSD: 12.932, (length-normalized): 1.2316, angle RMSD: 175.55\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 002: Train Loss: 23.22, dist RMSD: 16.348, (length-normalized): 1.4634, angle RMSD: 171.8 \n",
            "              Val Loss: 22.904, dist RMSD: 16.212, (length-normalized): 1.4877, angle RMSD: 167.3\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0003 Batch 000/087: Loss: 22.557, dist RMSD: 15.707, (length-normalized): 1.4359, angle RMSD: 171.23\n",
            "Epoch 0003 Batch 010/087: Loss: 21.339, dist RMSD: 14.482, (length-normalized): 1.2962, angle RMSD: 171.42\n",
            "Epoch 0003 Batch 020/087: Loss: 20.524, dist RMSD: 13.567, (length-normalized): 1.3467, angle RMSD: 173.91\n",
            "Epoch 0003 Batch 030/087: Loss: 21.962, dist RMSD: 15.04, (length-normalized): 1.3204, angle RMSD: 173.04\n",
            "Epoch 0003 Batch 040/087: Loss: 20.919, dist RMSD: 13.737, (length-normalized): 1.2043, angle RMSD: 179.57\n",
            "Epoch 0003 Batch 050/087: Loss: 23.506, dist RMSD: 16.566, (length-normalized): 1.3548, angle RMSD: 173.49\n",
            "Epoch 0003 Batch 060/087: Loss: 19.656, dist RMSD: 13.105, (length-normalized): 1.27, angle RMSD: 163.76\n",
            "Epoch 0003 Batch 070/087: Loss: 22.679, dist RMSD: 15.969, (length-normalized): 1.384, angle RMSD: 167.75\n",
            "Epoch 0003 Batch 080/087: Loss: 21.893, dist RMSD: 15.136, (length-normalized): 1.2577, angle RMSD: 168.92\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 003: Train Loss: 21.878, dist RMSD: 15.079, (length-normalized): 1.3515, angle RMSD: 169.98 \n",
            "              Val Loss: 20.987, dist RMSD: 14.436, (length-normalized): 1.3153, angle RMSD: 163.78\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0004 Batch 000/087: Loss: 20.132, dist RMSD: 13.6, (length-normalized): 1.2886, angle RMSD: 163.31\n",
            "Epoch 0004 Batch 010/087: Loss: 21.242, dist RMSD: 14.003, (length-normalized): 1.3337, angle RMSD: 180.96\n",
            "Epoch 0004 Batch 020/087: Loss: 20.763, dist RMSD: 14.198, (length-normalized): 1.3645, angle RMSD: 164.13\n",
            "Epoch 0004 Batch 030/087: Loss: 20.109, dist RMSD: 13.698, (length-normalized): 1.1437, angle RMSD: 160.27\n",
            "Epoch 0004 Batch 040/087: Loss: 20.9, dist RMSD: 14.697, (length-normalized): 1.334, angle RMSD: 155.08\n",
            "Epoch 0004 Batch 050/087: Loss: 20.827, dist RMSD: 14.398, (length-normalized): 1.2271, angle RMSD: 160.72\n",
            "Epoch 0004 Batch 060/087: Loss: 18.298, dist RMSD: 12.044, (length-normalized): 1.0677, angle RMSD: 156.34\n",
            "Epoch 0004 Batch 070/087: Loss: 59.036, dist RMSD: 55.288, (length-normalized): 4.3131, angle RMSD: 93.702\n",
            "Epoch 0004 Batch 080/087: Loss: 21.71, dist RMSD: 15.341, (length-normalized): 1.1833, angle RMSD: 159.24\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 004: Train Loss: 21.592, dist RMSD: 15.337, (length-normalized): 1.3697, angle RMSD: 156.38 \n",
            "              Val Loss: 21.567, dist RMSD: 15.306, (length-normalized): 1.3709, angle RMSD: 156.54\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0005 Batch 000/087: Loss: 21.51, dist RMSD: 14.872, (length-normalized): 1.3321, angle RMSD: 165.97\n",
            "Epoch 0005 Batch 010/087: Loss: 20.122, dist RMSD: 14.422, (length-normalized): 1.2653, angle RMSD: 142.49\n",
            "Epoch 0005 Batch 020/087: Loss: 21.22, dist RMSD: 16.162, (length-normalized): 1.4862, angle RMSD: 126.44\n",
            "Epoch 0005 Batch 030/087: Loss: 19.801, dist RMSD: 14.703, (length-normalized): 1.2304, angle RMSD: 127.47\n",
            "Epoch 0005 Batch 040/087: Loss: 18.685, dist RMSD: 14.105, (length-normalized): 1.3597, angle RMSD: 114.52\n",
            "Epoch 0005 Batch 050/087: Loss: 17.953, dist RMSD: 13.022, (length-normalized): 1.1806, angle RMSD: 123.29\n",
            "Epoch 0005 Batch 060/087: Loss: 20.85, dist RMSD: 16.094, (length-normalized): 1.387, angle RMSD: 118.88\n",
            "Epoch 0005 Batch 070/087: Loss: 19.589, dist RMSD: 14.353, (length-normalized): 1.1543, angle RMSD: 130.9\n",
            "Epoch 0005 Batch 080/087: Loss: 23.356, dist RMSD: 17.671, (length-normalized): 1.4371, angle RMSD: 142.14\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 005: Train Loss: 19.405, dist RMSD: 14.122, (length-normalized): 1.2538, angle RMSD: 132.08 \n",
            "              Val Loss: 19.089, dist RMSD: 13.594, (length-normalized): 1.2093, angle RMSD: 137.38\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0006 Batch 000/087: Loss: 19.549, dist RMSD: 13.929, (length-normalized): 1.1829, angle RMSD: 140.48\n",
            "Epoch 0006 Batch 010/087: Loss: 19.806, dist RMSD: 14.8, (length-normalized): 1.4186, angle RMSD: 125.13\n",
            "Epoch 0006 Batch 020/087: Loss: 18.756, dist RMSD: 12.966, (length-normalized): 0.99636, angle RMSD: 144.76\n",
            "Epoch 0006 Batch 030/087: Loss: 17.846, dist RMSD: 12.575, (length-normalized): 1.071, angle RMSD: 131.78\n",
            "Epoch 0006 Batch 040/087: Loss: 16.614, dist RMSD: 11.802, (length-normalized): 1.0916, angle RMSD: 120.29\n",
            "Epoch 0006 Batch 050/087: Loss: 19.967, dist RMSD: 15.301, (length-normalized): 1.3249, angle RMSD: 116.65\n",
            "Epoch 0006 Batch 060/087: Loss: 16.027, dist RMSD: 11.258, (length-normalized): 0.94124, angle RMSD: 119.21\n",
            "Epoch 0006 Batch 070/087: Loss: 17.467, dist RMSD: 13.119, (length-normalized): 1.2936, angle RMSD: 108.71\n",
            "Epoch 0006 Batch 080/087: Loss: 16.887, dist RMSD: 12.581, (length-normalized): 1.2764, angle RMSD: 107.63\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 006: Train Loss: 18.088, dist RMSD: 13.312, (length-normalized): 1.1739, angle RMSD: 119.4 \n",
            "              Val Loss: 17.515, dist RMSD: 12.903, (length-normalized): 1.1079, angle RMSD: 115.31\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0007 Batch 000/087: Loss: 16.634, dist RMSD: 11.93, (length-normalized): 1.0434, angle RMSD: 117.59\n",
            "Epoch 0007 Batch 010/087: Loss: 15.335, dist RMSD: 11.499, (length-normalized): 1.046, angle RMSD: 95.877\n",
            "Epoch 0007 Batch 020/087: Loss: 16.445, dist RMSD: 12.592, (length-normalized): 1.0526, angle RMSD: 96.337\n",
            "Epoch 0007 Batch 030/087: Loss: 13.265, dist RMSD: 9.7533, (length-normalized): 1.0026, angle RMSD: 87.788\n",
            "Epoch 0007 Batch 040/087: Loss: 16.849, dist RMSD: 13.264, (length-normalized): 1.1763, angle RMSD: 89.631\n",
            "Epoch 0007 Batch 050/087: Loss: 17.586, dist RMSD: 14.197, (length-normalized): 1.1486, angle RMSD: 84.728\n",
            "Epoch 0007 Batch 060/087: Loss: 16.647, dist RMSD: 13.206, (length-normalized): 1.0986, angle RMSD: 86.032\n",
            "Epoch 0007 Batch 070/087: Loss: 16.689, dist RMSD: 13.274, (length-normalized): 1.1248, angle RMSD: 85.373\n",
            "Epoch 0007 Batch 080/087: Loss: 16.112, dist RMSD: 12.663, (length-normalized): 1.0703, angle RMSD: 86.232\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 007: Train Loss: 16.455, dist RMSD: 12.884, (length-normalized): 1.1383, angle RMSD: 89.278 \n",
            "              Val Loss: 15.878, dist RMSD: 12.362, (length-normalized): 1.0629, angle RMSD: 87.898\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0008 Batch 000/087: Loss: 16.252, dist RMSD: 12.85, (length-normalized): 0.99435, angle RMSD: 85.059\n",
            "Epoch 0008 Batch 010/087: Loss: 15.446, dist RMSD: 11.801, (length-normalized): 1.0717, angle RMSD: 91.129\n",
            "Epoch 0008 Batch 020/087: Loss: 15.666, dist RMSD: 12.2, (length-normalized): 1.0885, angle RMSD: 86.652\n",
            "Epoch 0008 Batch 030/087: Loss: 16.267, dist RMSD: 12.559, (length-normalized): 1.0331, angle RMSD: 92.681\n",
            "Epoch 0008 Batch 040/087: Loss: 15.588, dist RMSD: 12.023, (length-normalized): 1.0786, angle RMSD: 89.13\n",
            "Epoch 0008 Batch 050/087: Loss: 14.994, dist RMSD: 11.52, (length-normalized): 1.0124, angle RMSD: 86.85\n",
            "Epoch 0008 Batch 060/087: Loss: 14.486, dist RMSD: 10.977, (length-normalized): 0.94701, angle RMSD: 87.732\n",
            "Epoch 0008 Batch 070/087: Loss: 18.922, dist RMSD: 15.328, (length-normalized): 1.3135, angle RMSD: 89.864\n",
            "Epoch 0008 Batch 080/087: Loss: 15.118, dist RMSD: 11.627, (length-normalized): 1.0476, angle RMSD: 87.288\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 008: Train Loss: 15.68, dist RMSD: 12.147, (length-normalized): 1.0774, angle RMSD: 88.322 \n",
            "              Val Loss: 15.4, dist RMSD: 11.932, (length-normalized): 1.0402, angle RMSD: 86.699\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0009 Batch 000/087: Loss: 14.338, dist RMSD: 10.802, (length-normalized): 1.0822, angle RMSD: 88.406\n",
            "Epoch 0009 Batch 010/087: Loss: 16.273, dist RMSD: 12.915, (length-normalized): 1.0965, angle RMSD: 83.953\n",
            "Epoch 0009 Batch 020/087: Loss: 13.691, dist RMSD: 10.267, (length-normalized): 0.89985, angle RMSD: 85.608\n",
            "Epoch 0009 Batch 030/087: Loss: 14.801, dist RMSD: 11.467, (length-normalized): 1.0223, angle RMSD: 83.361\n",
            "Epoch 0009 Batch 040/087: Loss: 15.079, dist RMSD: 11.699, (length-normalized): 0.99352, angle RMSD: 84.497\n",
            "Epoch 0009 Batch 050/087: Loss: 14.962, dist RMSD: 11.384, (length-normalized): 1.0655, angle RMSD: 89.437\n",
            "Epoch 0009 Batch 060/087: Loss: 15.627, dist RMSD: 12.206, (length-normalized): 1.0325, angle RMSD: 85.532\n",
            "Epoch 0009 Batch 070/087: Loss: 16.352, dist RMSD: 12.925, (length-normalized): 1.0301, angle RMSD: 85.68\n",
            "Epoch 0009 Batch 080/087: Loss: 15.524, dist RMSD: 12.036, (length-normalized): 1.1209, angle RMSD: 87.21\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 009: Train Loss: 15.126, dist RMSD: 11.695, (length-normalized): 1.0381, angle RMSD: 85.766 \n",
            "              Val Loss: 15.192, dist RMSD: 11.776, (length-normalized): 1.0292, angle RMSD: 85.385\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0010 Batch 000/087: Loss: 14.641, dist RMSD: 11.107, (length-normalized): 1.2173, angle RMSD: 88.365\n",
            "Epoch 0010 Batch 010/087: Loss: 14.544, dist RMSD: 11.252, (length-normalized): 1.0082, angle RMSD: 82.287\n",
            "Epoch 0010 Batch 020/087: Loss: 15.313, dist RMSD: 11.867, (length-normalized): 1.045, angle RMSD: 86.166\n",
            "Epoch 0010 Batch 030/087: Loss: 15.593, dist RMSD: 12.1, (length-normalized): 0.9683, angle RMSD: 87.314\n",
            "Epoch 0010 Batch 040/087: Loss: 14.268, dist RMSD: 10.938, (length-normalized): 0.87301, angle RMSD: 83.26\n",
            "Epoch 0010 Batch 050/087: Loss: 14.906, dist RMSD: 11.524, (length-normalized): 0.99451, angle RMSD: 84.55\n",
            "Epoch 0010 Batch 060/087: Loss: 16.185, dist RMSD: 12.696, (length-normalized): 1.0869, angle RMSD: 87.215\n",
            "Epoch 0010 Batch 070/087: Loss: 15.855, dist RMSD: 12.32, (length-normalized): 1.2284, angle RMSD: 88.376\n",
            "Epoch 0010 Batch 080/087: Loss: 15.856, dist RMSD: 12.523, (length-normalized): 1.1158, angle RMSD: 83.312\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 010: Train Loss: 15, dist RMSD: 11.593, (length-normalized): 1.0315, angle RMSD: 85.188 \n",
            "              Val Loss: 15.262, dist RMSD: 11.853, (length-normalized): 1.0409, angle RMSD: 85.212\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0011 Batch 000/087: Loss: 14.837, dist RMSD: 11.415, (length-normalized): 1.041, angle RMSD: 85.571\n",
            "Epoch 0011 Batch 010/087: Loss: 15.029, dist RMSD: 11.697, (length-normalized): 1.0423, angle RMSD: 83.31\n",
            "Epoch 0011 Batch 020/087: Loss: 13.817, dist RMSD: 10.275, (length-normalized): 0.94137, angle RMSD: 88.565\n",
            "Epoch 0011 Batch 030/087: Loss: 13.431, dist RMSD: 9.8736, (length-normalized): 0.93265, angle RMSD: 88.938\n",
            "Epoch 0011 Batch 040/087: Loss: 13.627, dist RMSD: 10.223, (length-normalized): 0.97489, angle RMSD: 85.11\n",
            "Epoch 0011 Batch 050/087: Loss: 14.236, dist RMSD: 10.853, (length-normalized): 1.1156, angle RMSD: 84.582\n",
            "Epoch 0011 Batch 060/087: Loss: 14.822, dist RMSD: 11.384, (length-normalized): 1.0037, angle RMSD: 85.941\n",
            "Epoch 0011 Batch 070/087: Loss: 14.838, dist RMSD: 11.474, (length-normalized): 0.96534, angle RMSD: 84.118\n",
            "Epoch 0011 Batch 080/087: Loss: 15.071, dist RMSD: 11.689, (length-normalized): 1.1346, angle RMSD: 84.561\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 011: Train Loss: 14.903, dist RMSD: 11.553, (length-normalized): 1.0279, angle RMSD: 83.742 \n",
            "              Val Loss: 15.244, dist RMSD: 11.917, (length-normalized): 1.0277, angle RMSD: 83.19\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0012 Batch 000/087: Loss: 13.423, dist RMSD: 10.067, (length-normalized): 0.8925, angle RMSD: 83.905\n",
            "Epoch 0012 Batch 010/087: Loss: 15.297, dist RMSD: 12.133, (length-normalized): 1.0576, angle RMSD: 79.101\n",
            "Epoch 0012 Batch 020/087: Loss: 14.784, dist RMSD: 11.452, (length-normalized): 0.9534, angle RMSD: 83.291\n",
            "Epoch 0012 Batch 030/087: Loss: 14.289, dist RMSD: 11.011, (length-normalized): 0.94771, angle RMSD: 81.949\n",
            "Epoch 0012 Batch 040/087: Loss: 16.211, dist RMSD: 13.044, (length-normalized): 1.1103, angle RMSD: 79.18\n",
            "Epoch 0012 Batch 050/087: Loss: 14.464, dist RMSD: 11.177, (length-normalized): 0.98109, angle RMSD: 82.166\n",
            "Epoch 0012 Batch 060/087: Loss: 15.665, dist RMSD: 12.378, (length-normalized): 1.121, angle RMSD: 82.191\n",
            "Epoch 0012 Batch 070/087: Loss: 14.237, dist RMSD: 10.99, (length-normalized): 0.93624, angle RMSD: 81.172\n",
            "Epoch 0012 Batch 080/087: Loss: 13.648, dist RMSD: 10.23, (length-normalized): 0.96497, angle RMSD: 85.439\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 012: Train Loss: 14.652, dist RMSD: 11.332, (length-normalized): 1.0119, angle RMSD: 83.007 \n",
            "              Val Loss: 14.633, dist RMSD: 11.321, (length-normalized): 0.99164, angle RMSD: 82.788\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0013 Batch 000/087: Loss: 15.61, dist RMSD: 12.299, (length-normalized): 1.0661, angle RMSD: 82.775\n",
            "Epoch 0013 Batch 010/087: Loss: 16.291, dist RMSD: 13.147, (length-normalized): 1.0842, angle RMSD: 78.605\n",
            "Epoch 0013 Batch 020/087: Loss: 13.686, dist RMSD: 10.336, (length-normalized): 0.94939, angle RMSD: 83.744\n",
            "Epoch 0013 Batch 030/087: Loss: 13.562, dist RMSD: 10.171, (length-normalized): 0.90758, angle RMSD: 84.788\n",
            "Epoch 0013 Batch 040/087: Loss: 14.975, dist RMSD: 11.642, (length-normalized): 1.0765, angle RMSD: 83.322\n",
            "Epoch 0013 Batch 050/087: Loss: 13.707, dist RMSD: 10.465, (length-normalized): 0.92045, angle RMSD: 81.053\n",
            "Epoch 0013 Batch 060/087: Loss: 15.078, dist RMSD: 11.681, (length-normalized): 0.99649, angle RMSD: 84.916\n",
            "Epoch 0013 Batch 070/087: Loss: 14.879, dist RMSD: 11.47, (length-normalized): 1.0457, angle RMSD: 85.217\n",
            "Epoch 0013 Batch 080/087: Loss: 14.905, dist RMSD: 11.712, (length-normalized): 1.0713, angle RMSD: 79.816\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 013: Train Loss: 14.552, dist RMSD: 11.233, (length-normalized): 1.0018, angle RMSD: 82.964 \n",
            "              Val Loss: 14.45, dist RMSD: 11.14, (length-normalized): 0.98152, angle RMSD: 82.747\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0014 Batch 000/087: Loss: 13.186, dist RMSD: 9.88, (length-normalized): 0.96364, angle RMSD: 82.648\n",
            "Epoch 0014 Batch 010/087: Loss: 14.28, dist RMSD: 10.943, (length-normalized): 1.058, angle RMSD: 83.44\n",
            "Epoch 0014 Batch 020/087: Loss: 13.793, dist RMSD: 10.643, (length-normalized): 0.90875, angle RMSD: 78.758\n",
            "Epoch 0014 Batch 030/087: Loss: 13.485, dist RMSD: 10.036, (length-normalized): 0.98377, angle RMSD: 86.225\n",
            "Epoch 0014 Batch 040/087: Loss: 13.243, dist RMSD: 9.9741, (length-normalized): 0.98993, angle RMSD: 81.725\n",
            "Epoch 0014 Batch 050/087: Loss: 14.999, dist RMSD: 11.639, (length-normalized): 1.0234, angle RMSD: 84.012\n",
            "Epoch 0014 Batch 060/087: Loss: 14.637, dist RMSD: 11.201, (length-normalized): 1.1401, angle RMSD: 85.901\n",
            "Epoch 0014 Batch 070/087: Loss: 13.739, dist RMSD: 10.421, (length-normalized): 0.94509, angle RMSD: 82.959\n",
            "Epoch 0014 Batch 080/087: Loss: 14.149, dist RMSD: 10.807, (length-normalized): 0.95199, angle RMSD: 83.567\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 014: Train Loss: 14.467, dist RMSD: 11.146, (length-normalized): 0.99338, angle RMSD: 83.011 \n",
            "              Val Loss: 14.75, dist RMSD: 11.427, (length-normalized): 1.0053, angle RMSD: 83.077\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0015 Batch 000/087: Loss: 13.76, dist RMSD: 10.455, (length-normalized): 0.94668, angle RMSD: 82.63\n",
            "Epoch 0015 Batch 010/087: Loss: 16.386, dist RMSD: 13.047, (length-normalized): 1.0763, angle RMSD: 83.473\n",
            "Epoch 0015 Batch 020/087: Loss: 14.645, dist RMSD: 11.343, (length-normalized): 0.93941, angle RMSD: 82.555\n",
            "Epoch 0015 Batch 030/087: Loss: 15.729, dist RMSD: 12.503, (length-normalized): 1.0127, angle RMSD: 80.644\n",
            "Epoch 0015 Batch 040/087: Loss: 14.958, dist RMSD: 11.688, (length-normalized): 0.99809, angle RMSD: 81.752\n",
            "Epoch 0015 Batch 050/087: Loss: 13.859, dist RMSD: 10.349, (length-normalized): 0.94025, angle RMSD: 87.732\n",
            "Epoch 0015 Batch 060/087: Loss: 14.323, dist RMSD: 11.109, (length-normalized): 0.99013, angle RMSD: 80.365\n",
            "Epoch 0015 Batch 070/087: Loss: 13.568, dist RMSD: 10.346, (length-normalized): 0.94354, angle RMSD: 80.545\n",
            "Epoch 0015 Batch 080/087: Loss: 13.684, dist RMSD: 10.272, (length-normalized): 0.94497, angle RMSD: 85.308\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 015: Train Loss: 14.354, dist RMSD: 11.029, (length-normalized): 0.98632, angle RMSD: 83.135 \n",
            "              Val Loss: 14.559, dist RMSD: 11.245, (length-normalized): 0.98261, angle RMSD: 82.859\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0016 Batch 000/087: Loss: 13.784, dist RMSD: 10.302, (length-normalized): 0.98971, angle RMSD: 87.045\n",
            "Epoch 0016 Batch 010/087: Loss: 14.118, dist RMSD: 10.806, (length-normalized): 0.9415, angle RMSD: 82.811\n",
            "Epoch 0016 Batch 020/087: Loss: 13.386, dist RMSD: 9.9382, (length-normalized): 0.96236, angle RMSD: 86.186\n",
            "Epoch 0016 Batch 030/087: Loss: 13.789, dist RMSD: 10.54, (length-normalized): 0.91584, angle RMSD: 81.232\n",
            "Epoch 0016 Batch 040/087: Loss: 15.428, dist RMSD: 12.172, (length-normalized): 0.99745, angle RMSD: 81.412\n",
            "Epoch 0016 Batch 050/087: Loss: 14.225, dist RMSD: 10.855, (length-normalized): 1.0663, angle RMSD: 84.25\n",
            "Epoch 0016 Batch 060/087: Loss: 12.793, dist RMSD: 9.4376, (length-normalized): 0.89956, angle RMSD: 83.887\n",
            "Epoch 0016 Batch 070/087: Loss: 13.609, dist RMSD: 10.228, (length-normalized): 1.003, angle RMSD: 84.519\n",
            "Epoch 0016 Batch 080/087: Loss: 14.787, dist RMSD: 11.554, (length-normalized): 1.0093, angle RMSD: 80.823\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 016: Train Loss: 14.351, dist RMSD: 11.035, (length-normalized): 0.98658, angle RMSD: 82.898 \n",
            "              Val Loss: 14.823, dist RMSD: 11.529, (length-normalized): 1.0055, angle RMSD: 82.35\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0017 Batch 000/087: Loss: 15.391, dist RMSD: 12.182, (length-normalized): 0.9922, angle RMSD: 80.236\n",
            "Epoch 0017 Batch 010/087: Loss: 13.01, dist RMSD: 9.5294, (length-normalized): 0.85293, angle RMSD: 87.008\n",
            "Epoch 0017 Batch 020/087: Loss: 13.577, dist RMSD: 10.218, (length-normalized): 0.92259, angle RMSD: 83.974\n",
            "Epoch 0017 Batch 030/087: Loss: 14.368, dist RMSD: 11.113, (length-normalized): 1.078, angle RMSD: 81.359\n",
            "Epoch 0017 Batch 040/087: Loss: 18.027, dist RMSD: 14.7, (length-normalized): 1.1794, angle RMSD: 83.172\n",
            "Epoch 0017 Batch 050/087: Loss: 14.994, dist RMSD: 11.468, (length-normalized): 0.93735, angle RMSD: 88.134\n",
            "Epoch 0017 Batch 060/087: Loss: 13.602, dist RMSD: 10.236, (length-normalized): 0.9446, angle RMSD: 84.146\n",
            "Epoch 0017 Batch 070/087: Loss: 12.425, dist RMSD: 9.1846, (length-normalized): 0.89198, angle RMSD: 81.012\n",
            "Epoch 0017 Batch 080/087: Loss: 13.696, dist RMSD: 10.39, (length-normalized): 0.94228, angle RMSD: 82.647\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 017: Train Loss: 14.228, dist RMSD: 10.916, (length-normalized): 0.97614, angle RMSD: 82.798 \n",
            "              Val Loss: 15.027, dist RMSD: 11.726, (length-normalized): 1.0062, angle RMSD: 82.526\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0018 Batch 000/087: Loss: 15.401, dist RMSD: 12.017, (length-normalized): 1.0676, angle RMSD: 84.613\n",
            "Epoch 0018 Batch 010/087: Loss: 14.582, dist RMSD: 11.425, (length-normalized): 0.98174, angle RMSD: 78.905\n",
            "Epoch 0018 Batch 020/087: Loss: 15.066, dist RMSD: 11.719, (length-normalized): 1.0337, angle RMSD: 83.665\n",
            "Epoch 0018 Batch 030/087: Loss: 12.955, dist RMSD: 9.7274, (length-normalized): 0.87885, angle RMSD: 80.699\n",
            "Epoch 0018 Batch 040/087: Loss: 15.065, dist RMSD: 11.731, (length-normalized): 1.0063, angle RMSD: 83.332\n",
            "Epoch 0018 Batch 050/087: Loss: 13.966, dist RMSD: 10.73, (length-normalized): 0.94179, angle RMSD: 80.904\n",
            "Epoch 0018 Batch 060/087: Loss: 14.194, dist RMSD: 10.927, (length-normalized): 0.99839, angle RMSD: 81.682\n",
            "Epoch 0018 Batch 070/087: Loss: 12.819, dist RMSD: 9.4376, (length-normalized): 0.95273, angle RMSD: 84.523\n",
            "Epoch 0018 Batch 080/087: Loss: 13.805, dist RMSD: 10.588, (length-normalized): 0.96626, angle RMSD: 80.422\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 018: Train Loss: 14.202, dist RMSD: 10.897, (length-normalized): 0.97562, angle RMSD: 82.629 \n",
            "              Val Loss: 14.298, dist RMSD: 11.008, (length-normalized): 0.96852, angle RMSD: 82.246\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0019 Batch 000/087: Loss: 15.082, dist RMSD: 11.817, (length-normalized): 1.0299, angle RMSD: 81.617\n",
            "Epoch 0019 Batch 010/087: Loss: 13.948, dist RMSD: 10.549, (length-normalized): 0.955, angle RMSD: 84.969\n",
            "Epoch 0019 Batch 020/087: Loss: 12.716, dist RMSD: 9.3963, (length-normalized): 0.82262, angle RMSD: 82.99\n",
            "Epoch 0019 Batch 030/087: Loss: 14.917, dist RMSD: 11.645, (length-normalized): 1.0248, angle RMSD: 81.797\n",
            "Epoch 0019 Batch 040/087: Loss: 15.141, dist RMSD: 11.854, (length-normalized): 1.0954, angle RMSD: 82.156\n",
            "Epoch 0019 Batch 050/087: Loss: 12.359, dist RMSD: 8.8562, (length-normalized): 0.98685, angle RMSD: 87.576\n",
            "Epoch 0019 Batch 060/087: Loss: 15.575, dist RMSD: 12.123, (length-normalized): 1.2027, angle RMSD: 86.298\n",
            "Epoch 0019 Batch 070/087: Loss: 13.4, dist RMSD: 10.176, (length-normalized): 0.91238, angle RMSD: 80.61\n",
            "Epoch 0019 Batch 080/087: Loss: 13.158, dist RMSD: 9.8091, (length-normalized): 0.96019, angle RMSD: 83.728\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 019: Train Loss: 14.15, dist RMSD: 10.844, (length-normalized): 0.96816, angle RMSD: 82.629 \n",
            "              Val Loss: 14.158, dist RMSD: 10.871, (length-normalized): 0.95778, angle RMSD: 82.163\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0020 Batch 000/087: Loss: 13.629, dist RMSD: 10.254, (length-normalized): 0.97551, angle RMSD: 84.383\n",
            "Epoch 0020 Batch 010/087: Loss: 13.885, dist RMSD: 10.574, (length-normalized): 0.94, angle RMSD: 82.768\n",
            "Epoch 0020 Batch 020/087: Loss: 13.333, dist RMSD: 10.022, (length-normalized): 0.9696, angle RMSD: 82.768\n",
            "Epoch 0020 Batch 030/087: Loss: 12.301, dist RMSD: 8.9403, (length-normalized): 0.83827, angle RMSD: 84.013\n",
            "Epoch 0020 Batch 040/087: Loss: 14.367, dist RMSD: 10.975, (length-normalized): 0.95997, angle RMSD: 84.793\n",
            "Epoch 0020 Batch 050/087: Loss: 17.225, dist RMSD: 13.91, (length-normalized): 1.1713, angle RMSD: 82.87\n",
            "Epoch 0020 Batch 060/087: Loss: 13.828, dist RMSD: 10.819, (length-normalized): 0.86219, angle RMSD: 75.226\n",
            "Epoch 0020 Batch 070/087: Loss: 14.336, dist RMSD: 11.02, (length-normalized): 0.99837, angle RMSD: 82.916\n",
            "Epoch 0020 Batch 080/087: Loss: 13.299, dist RMSD: 10.159, (length-normalized): 0.83653, angle RMSD: 78.514\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 020: Train Loss: 14.126, dist RMSD: 10.829, (length-normalized): 0.96721, angle RMSD: 82.431 \n",
            "              Val Loss: 14.182, dist RMSD: 10.901, (length-normalized): 0.95969, angle RMSD: 82.033\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0021 Batch 000/087: Loss: 13.062, dist RMSD: 9.8583, (length-normalized): 0.87064, angle RMSD: 80.088\n",
            "Epoch 0021 Batch 010/087: Loss: 14.037, dist RMSD: 10.772, (length-normalized): 0.95319, angle RMSD: 81.621\n",
            "Epoch 0021 Batch 020/087: Loss: 13.729, dist RMSD: 10.365, (length-normalized): 0.97751, angle RMSD: 84.1\n",
            "Epoch 0021 Batch 030/087: Loss: 14.178, dist RMSD: 10.969, (length-normalized): 0.97635, angle RMSD: 80.224\n",
            "Epoch 0021 Batch 040/087: Loss: 13.412, dist RMSD: 10.155, (length-normalized): 0.88738, angle RMSD: 81.42\n",
            "Epoch 0021 Batch 050/087: Loss: 14.135, dist RMSD: 10.783, (length-normalized): 0.98099, angle RMSD: 83.812\n",
            "Epoch 0021 Batch 060/087: Loss: 13.103, dist RMSD: 9.7394, (length-normalized): 0.94313, angle RMSD: 84.083\n",
            "Epoch 0021 Batch 070/087: Loss: 14.596, dist RMSD: 11.102, (length-normalized): 0.99414, angle RMSD: 87.355\n",
            "Epoch 0021 Batch 080/087: Loss: 13.597, dist RMSD: 10.159, (length-normalized): 0.92274, angle RMSD: 85.937\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 021: Train Loss: 14.063, dist RMSD: 10.772, (length-normalized): 0.96285, angle RMSD: 82.278 \n",
            "              Val Loss: 14.281, dist RMSD: 11.003, (length-normalized): 0.96713, angle RMSD: 81.951\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0022 Batch 000/087: Loss: 13.265, dist RMSD: 9.99, (length-normalized): 0.83783, angle RMSD: 81.882\n",
            "Epoch 0022 Batch 010/087: Loss: 13.95, dist RMSD: 10.593, (length-normalized): 0.91941, angle RMSD: 83.917\n",
            "Epoch 0022 Batch 020/087: Loss: 13.012, dist RMSD: 9.7717, (length-normalized): 0.86643, angle RMSD: 80.998\n",
            "Epoch 0022 Batch 030/087: Loss: 14.085, dist RMSD: 10.987, (length-normalized): 0.89, angle RMSD: 77.446\n",
            "Epoch 0022 Batch 040/087: Loss: 14.44, dist RMSD: 11.058, (length-normalized): 0.97095, angle RMSD: 84.546\n",
            "Epoch 0022 Batch 050/087: Loss: 14.997, dist RMSD: 11.787, (length-normalized): 0.94275, angle RMSD: 80.239\n",
            "Epoch 0022 Batch 060/087: Loss: 13.673, dist RMSD: 10.447, (length-normalized): 0.89776, angle RMSD: 80.656\n",
            "Epoch 0022 Batch 070/087: Loss: 12.745, dist RMSD: 9.5193, (length-normalized): 0.9443, angle RMSD: 80.651\n",
            "Epoch 0022 Batch 080/087: Loss: 14.584, dist RMSD: 11.252, (length-normalized): 0.87053, angle RMSD: 83.305\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 022: Train Loss: 13.999, dist RMSD: 10.714, (length-normalized): 0.95741, angle RMSD: 82.108 \n",
            "              Val Loss: 14.341, dist RMSD: 11.077, (length-normalized): 0.96975, angle RMSD: 81.608\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0023 Batch 000/087: Loss: 14.444, dist RMSD: 11.101, (length-normalized): 0.98788, angle RMSD: 83.589\n",
            "Epoch 0023 Batch 010/087: Loss: 13.855, dist RMSD: 10.636, (length-normalized): 0.92703, angle RMSD: 80.487\n",
            "Epoch 0023 Batch 020/087: Loss: 11.922, dist RMSD: 8.6872, (length-normalized): 0.86508, angle RMSD: 80.86\n",
            "Epoch 0023 Batch 030/087: Loss: 12.698, dist RMSD: 9.3377, (length-normalized): 0.87892, angle RMSD: 84.018\n",
            "Epoch 0023 Batch 040/087: Loss: 13.986, dist RMSD: 10.62, (length-normalized): 0.91976, angle RMSD: 84.15\n",
            "Epoch 0023 Batch 050/087: Loss: 15.069, dist RMSD: 11.727, (length-normalized): 0.96287, angle RMSD: 83.547\n",
            "Epoch 0023 Batch 060/087: Loss: 14.586, dist RMSD: 11.149, (length-normalized): 0.91383, angle RMSD: 85.93\n",
            "Epoch 0023 Batch 070/087: Loss: 14.036, dist RMSD: 10.676, (length-normalized): 0.84219, angle RMSD: 83.98\n",
            "Epoch 0023 Batch 080/087: Loss: 13.037, dist RMSD: 9.684, (length-normalized): 0.96903, angle RMSD: 83.832\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 023: Train Loss: 13.928, dist RMSD: 10.648, (length-normalized): 0.952, angle RMSD: 81.987 \n",
            "              Val Loss: 14.563, dist RMSD: 11.305, (length-normalized): 0.98535, angle RMSD: 81.452\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0024 Batch 000/087: Loss: 13.522, dist RMSD: 10.312, (length-normalized): 0.95153, angle RMSD: 80.242\n",
            "Epoch 0024 Batch 010/087: Loss: 12.511, dist RMSD: 9.2061, (length-normalized): 0.88716, angle RMSD: 82.619\n",
            "Epoch 0024 Batch 020/087: Loss: 13.283, dist RMSD: 10.041, (length-normalized): 0.89364, angle RMSD: 81.046\n",
            "Epoch 0024 Batch 030/087: Loss: 13.023, dist RMSD: 9.7405, (length-normalized): 0.94295, angle RMSD: 82.063\n",
            "Epoch 0024 Batch 040/087: Loss: 14.537, dist RMSD: 11.194, (length-normalized): 1.1247, angle RMSD: 83.58\n",
            "Epoch 0024 Batch 050/087: Loss: 13.704, dist RMSD: 10.401, (length-normalized): 0.92316, angle RMSD: 82.58\n",
            "Epoch 0024 Batch 060/087: Loss: 12.843, dist RMSD: 9.5757, (length-normalized): 0.87159, angle RMSD: 81.67\n",
            "Epoch 0024 Batch 070/087: Loss: 15.522, dist RMSD: 12.277, (length-normalized): 1.0315, angle RMSD: 81.101\n",
            "Epoch 0024 Batch 080/087: Loss: 13.567, dist RMSD: 10.352, (length-normalized): 0.84197, angle RMSD: 80.375\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 024: Train Loss: 13.945, dist RMSD: 10.669, (length-normalized): 0.95206, angle RMSD: 81.908 \n",
            "              Val Loss: 14.064, dist RMSD: 10.803, (length-normalized): 0.94936, angle RMSD: 81.517\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0025 Batch 000/087: Loss: 14.695, dist RMSD: 11.441, (length-normalized): 1.0734, angle RMSD: 81.362\n",
            "Epoch 0025 Batch 010/087: Loss: 13.978, dist RMSD: 10.737, (length-normalized): 0.95718, angle RMSD: 81.025\n",
            "Epoch 0025 Batch 020/087: Loss: 14.068, dist RMSD: 10.809, (length-normalized): 0.89208, angle RMSD: 81.472\n",
            "Epoch 0025 Batch 030/087: Loss: 13.759, dist RMSD: 10.357, (length-normalized): 0.93111, angle RMSD: 85.047\n",
            "Epoch 0025 Batch 040/087: Loss: 15.169, dist RMSD: 11.97, (length-normalized): 1.0026, angle RMSD: 79.995\n",
            "Epoch 0025 Batch 050/087: Loss: 13.651, dist RMSD: 10.31, (length-normalized): 0.9137, angle RMSD: 83.537\n",
            "Epoch 0025 Batch 060/087: Loss: 13.455, dist RMSD: 10.175, (length-normalized): 0.91568, angle RMSD: 81.984\n",
            "Epoch 0025 Batch 070/087: Loss: 14.545, dist RMSD: 11.318, (length-normalized): 0.99024, angle RMSD: 80.688\n",
            "Epoch 0025 Batch 080/087: Loss: 14.76, dist RMSD: 11.508, (length-normalized): 1.0658, angle RMSD: 81.319\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 025: Train Loss: 13.93, dist RMSD: 10.659, (length-normalized): 0.95021, angle RMSD: 81.784 \n",
            "              Val Loss: 14.3, dist RMSD: 11.036, (length-normalized): 0.95695, angle RMSD: 81.608\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0026 Batch 000/087: Loss: 15.195, dist RMSD: 11.824, (length-normalized): 0.90411, angle RMSD: 84.279\n",
            "Epoch 0026 Batch 010/087: Loss: 14.088, dist RMSD: 10.756, (length-normalized): 0.93188, angle RMSD: 83.302\n",
            "Epoch 0026 Batch 020/087: Loss: 13.187, dist RMSD: 9.8118, (length-normalized): 0.8931, angle RMSD: 84.386\n",
            "Epoch 0026 Batch 030/087: Loss: 13.128, dist RMSD: 9.9382, (length-normalized): 0.80726, angle RMSD: 79.735\n",
            "Epoch 0026 Batch 040/087: Loss: 13.307, dist RMSD: 10.154, (length-normalized): 0.88347, angle RMSD: 78.823\n",
            "Epoch 0026 Batch 050/087: Loss: 12.8, dist RMSD: 9.4921, (length-normalized): 0.84448, angle RMSD: 82.688\n",
            "Epoch 0026 Batch 060/087: Loss: 12.869, dist RMSD: 9.7736, (length-normalized): 0.88328, angle RMSD: 77.392\n",
            "Epoch 0026 Batch 070/087: Loss: 13.702, dist RMSD: 10.468, (length-normalized): 0.92793, angle RMSD: 80.827\n",
            "Epoch 0026 Batch 080/087: Loss: 14.023, dist RMSD: 10.803, (length-normalized): 0.98388, angle RMSD: 80.494\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 026: Train Loss: 13.992, dist RMSD: 10.725, (length-normalized): 0.95404, angle RMSD: 81.665 \n",
            "              Val Loss: 14.27, dist RMSD: 11.022, (length-normalized): 0.96161, angle RMSD: 81.209\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0027 Batch 000/087: Loss: 13.823, dist RMSD: 10.453, (length-normalized): 0.94127, angle RMSD: 84.234\n",
            "Epoch 0027 Batch 010/087: Loss: 15.414, dist RMSD: 12.131, (length-normalized): 1.0605, angle RMSD: 82.085\n",
            "Epoch 0027 Batch 020/087: Loss: 14.552, dist RMSD: 11.208, (length-normalized): 0.97009, angle RMSD: 83.605\n",
            "Epoch 0027 Batch 030/087: Loss: 15.437, dist RMSD: 12.111, (length-normalized): 1.0552, angle RMSD: 83.151\n",
            "Epoch 0027 Batch 040/087: Loss: 14.233, dist RMSD: 10.871, (length-normalized): 0.92137, angle RMSD: 84.049\n",
            "Epoch 0027 Batch 050/087: Loss: 14.07, dist RMSD: 10.883, (length-normalized): 0.88625, angle RMSD: 79.681\n",
            "Epoch 0027 Batch 060/087: Loss: 13.959, dist RMSD: 10.904, (length-normalized): 0.90092, angle RMSD: 76.368\n",
            "Epoch 0027 Batch 070/087: Loss: 15.214, dist RMSD: 12.054, (length-normalized): 0.99677, angle RMSD: 79.013\n",
            "Epoch 0027 Batch 080/087: Loss: 14.64, dist RMSD: 11.344, (length-normalized): 1.0951, angle RMSD: 82.415\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 027: Train Loss: 13.935, dist RMSD: 10.68, (length-normalized): 0.95081, angle RMSD: 81.37 \n",
            "              Val Loss: 14.215, dist RMSD: 10.963, (length-normalized): 0.95341, angle RMSD: 81.285\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0028 Batch 000/087: Loss: 14.503, dist RMSD: 11.318, (length-normalized): 0.88558, angle RMSD: 79.623\n",
            "Epoch 0028 Batch 010/087: Loss: 12.958, dist RMSD: 9.6761, (length-normalized): 0.95307, angle RMSD: 82.04\n",
            "Epoch 0028 Batch 020/087: Loss: 12.528, dist RMSD: 9.4725, (length-normalized): 0.82771, angle RMSD: 76.387\n",
            "Epoch 0028 Batch 030/087: Loss: 14.574, dist RMSD: 11.457, (length-normalized): 0.9056, angle RMSD: 77.935\n",
            "Epoch 0028 Batch 040/087: Loss: 14.155, dist RMSD: 10.893, (length-normalized): 0.9632, angle RMSD: 81.545\n",
            "Epoch 0028 Batch 050/087: Loss: 12.714, dist RMSD: 9.6441, (length-normalized): 0.82829, angle RMSD: 76.75\n",
            "Epoch 0028 Batch 060/087: Loss: 13.204, dist RMSD: 10.028, (length-normalized): 0.93863, angle RMSD: 79.402\n",
            "Epoch 0028 Batch 070/087: Loss: 14.431, dist RMSD: 11.384, (length-normalized): 0.92524, angle RMSD: 76.163\n",
            "Epoch 0028 Batch 080/087: Loss: 12.612, dist RMSD: 9.1514, (length-normalized): 0.84714, angle RMSD: 86.503\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 028: Train Loss: 13.809, dist RMSD: 10.557, (length-normalized): 0.94133, angle RMSD: 81.317 \n",
            "              Val Loss: 13.876, dist RMSD: 10.639, (length-normalized): 0.93125, angle RMSD: 80.904\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0029 Batch 000/087: Loss: 14.683, dist RMSD: 11.53, (length-normalized): 0.9466, angle RMSD: 78.831\n",
            "Epoch 0029 Batch 010/087: Loss: 15.638, dist RMSD: 12.397, (length-normalized): 1.0374, angle RMSD: 81.019\n",
            "Epoch 0029 Batch 020/087: Loss: 15.075, dist RMSD: 11.717, (length-normalized): 1.0272, angle RMSD: 83.933\n",
            "Epoch 0029 Batch 030/087: Loss: 16.386, dist RMSD: 13.11, (length-normalized): 1.0507, angle RMSD: 81.915\n",
            "Epoch 0029 Batch 040/087: Loss: 13.277, dist RMSD: 10.186, (length-normalized): 0.8113, angle RMSD: 77.283\n",
            "Epoch 0029 Batch 050/087: Loss: 13.569, dist RMSD: 10.33, (length-normalized): 0.93174, angle RMSD: 80.959\n",
            "Epoch 0029 Batch 060/087: Loss: 13.937, dist RMSD: 10.694, (length-normalized): 0.92802, angle RMSD: 81.087\n",
            "Epoch 0029 Batch 070/087: Loss: 13.572, dist RMSD: 10.369, (length-normalized): 0.91329, angle RMSD: 80.083\n",
            "Epoch 0029 Batch 080/087: Loss: 13.558, dist RMSD: 10.255, (length-normalized): 0.99872, angle RMSD: 82.566\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 029: Train Loss: 13.742, dist RMSD: 10.5, (length-normalized): 0.93584, angle RMSD: 81.049 \n",
            "              Val Loss: 13.858, dist RMSD: 10.63, (length-normalized): 0.93369, angle RMSD: 80.714\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0030 Batch 000/087: Loss: 13.809, dist RMSD: 10.388, (length-normalized): 0.94284, angle RMSD: 85.527\n",
            "Epoch 0030 Batch 010/087: Loss: 13.71, dist RMSD: 10.426, (length-normalized): 0.92961, angle RMSD: 82.099\n",
            "Epoch 0030 Batch 020/087: Loss: 12.403, dist RMSD: 9.0224, (length-normalized): 0.8041, angle RMSD: 84.517\n",
            "Epoch 0030 Batch 030/087: Loss: 13.79, dist RMSD: 10.559, (length-normalized): 0.88743, angle RMSD: 80.774\n",
            "Epoch 0030 Batch 040/087: Loss: 14.892, dist RMSD: 11.641, (length-normalized): 1.0155, angle RMSD: 81.287\n",
            "Epoch 0030 Batch 050/087: Loss: 14.709, dist RMSD: 11.57, (length-normalized): 1.0548, angle RMSD: 78.493\n",
            "Epoch 0030 Batch 060/087: Loss: 13.403, dist RMSD: 10.23, (length-normalized): 0.89515, angle RMSD: 79.331\n",
            "Epoch 0030 Batch 070/087: Loss: 13.189, dist RMSD: 9.867, (length-normalized): 0.77329, angle RMSD: 83.052\n",
            "Epoch 0030 Batch 080/087: Loss: 12.624, dist RMSD: 9.5996, (length-normalized): 0.8939, angle RMSD: 75.604\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 030: Train Loss: 13.725, dist RMSD: 10.489, (length-normalized): 0.93228, angle RMSD: 80.908 \n",
            "              Val Loss: 13.915, dist RMSD: 10.694, (length-normalized): 0.9354, angle RMSD: 80.525\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0031 Batch 000/087: Loss: 12.914, dist RMSD: 9.8833, (length-normalized): 0.86354, angle RMSD: 75.772\n",
            "Epoch 0031 Batch 010/087: Loss: 13.798, dist RMSD: 10.704, (length-normalized): 0.97369, angle RMSD: 77.339\n",
            "Epoch 0031 Batch 020/087: Loss: 12.117, dist RMSD: 8.7842, (length-normalized): 0.86135, angle RMSD: 83.327\n",
            "Epoch 0031 Batch 030/087: Loss: 14.966, dist RMSD: 11.622, (length-normalized): 1.0169, angle RMSD: 83.606\n",
            "Epoch 0031 Batch 040/087: Loss: 15.186, dist RMSD: 12.073, (length-normalized): 1.0776, angle RMSD: 77.836\n",
            "Epoch 0031 Batch 050/087: Loss: 15.661, dist RMSD: 12.417, (length-normalized): 1.0455, angle RMSD: 81.094\n",
            "Epoch 0031 Batch 060/087: Loss: 14.306, dist RMSD: 11.085, (length-normalized): 0.97528, angle RMSD: 80.529\n",
            "Epoch 0031 Batch 070/087: Loss: 12.842, dist RMSD: 9.5407, (length-normalized): 0.82707, angle RMSD: 82.525\n",
            "Epoch 0031 Batch 080/087: Loss: 14.138, dist RMSD: 11.064, (length-normalized): 0.86188, angle RMSD: 76.85\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 031: Train Loss: 13.646, dist RMSD: 10.419, (length-normalized): 0.92749, angle RMSD: 80.677 \n",
            "              Val Loss: 13.852, dist RMSD: 10.639, (length-normalized): 0.93051, angle RMSD: 80.313\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0032 Batch 000/087: Loss: 13.958, dist RMSD: 10.678, (length-normalized): 0.91036, angle RMSD: 82.007\n",
            "Epoch 0032 Batch 010/087: Loss: 12.641, dist RMSD: 9.4717, (length-normalized): 0.79232, angle RMSD: 79.239\n",
            "Epoch 0032 Batch 020/087: Loss: 14.821, dist RMSD: 11.467, (length-normalized): 1.0048, angle RMSD: 83.864\n",
            "Epoch 0032 Batch 030/087: Loss: 14.751, dist RMSD: 11.69, (length-normalized): 1.007, angle RMSD: 76.513\n",
            "Epoch 0032 Batch 040/087: Loss: 12.583, dist RMSD: 9.4452, (length-normalized): 0.83302, angle RMSD: 78.443\n",
            "Epoch 0032 Batch 050/087: Loss: 12.609, dist RMSD: 9.4303, (length-normalized): 0.83142, angle RMSD: 79.47\n",
            "Epoch 0032 Batch 060/087: Loss: 14.974, dist RMSD: 11.729, (length-normalized): 1.0264, angle RMSD: 81.125\n",
            "Epoch 0032 Batch 070/087: Loss: 12.908, dist RMSD: 9.7195, (length-normalized): 0.85833, angle RMSD: 79.724\n",
            "Epoch 0032 Batch 080/087: Loss: 13.899, dist RMSD: 10.713, (length-normalized): 0.8756, angle RMSD: 79.653\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 032: Train Loss: 13.58, dist RMSD: 10.36, (length-normalized): 0.92124, angle RMSD: 80.513 \n",
            "              Val Loss: 14.084, dist RMSD: 10.885, (length-normalized): 0.94656, angle RMSD: 79.987\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0033 Batch 000/087: Loss: 12.937, dist RMSD: 9.7657, (length-normalized): 0.88678, angle RMSD: 79.285\n",
            "Epoch 0033 Batch 010/087: Loss: 14.331, dist RMSD: 11.153, (length-normalized): 0.98224, angle RMSD: 79.457\n",
            "Epoch 0033 Batch 020/087: Loss: 12.348, dist RMSD: 9.0326, (length-normalized): 0.82425, angle RMSD: 82.884\n",
            "Epoch 0033 Batch 030/087: Loss: 14.345, dist RMSD: 11.08, (length-normalized): 0.97663, angle RMSD: 81.632\n",
            "Epoch 0033 Batch 040/087: Loss: 14.383, dist RMSD: 11.221, (length-normalized): 1.0157, angle RMSD: 79.048\n",
            "Epoch 0033 Batch 050/087: Loss: 13.388, dist RMSD: 10.114, (length-normalized): 0.94892, angle RMSD: 81.865\n",
            "Epoch 0033 Batch 060/087: Loss: 14.68, dist RMSD: 11.37, (length-normalized): 0.96226, angle RMSD: 82.756\n",
            "Epoch 0033 Batch 070/087: Loss: 12.742, dist RMSD: 9.4511, (length-normalized): 0.91617, angle RMSD: 82.261\n",
            "Epoch 0033 Batch 080/087: Loss: 12.889, dist RMSD: 9.7598, (length-normalized): 0.83284, angle RMSD: 78.239\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 033: Train Loss: 13.514, dist RMSD: 10.301, (length-normalized): 0.91443, angle RMSD: 80.326 \n",
            "              Val Loss: 13.621, dist RMSD: 10.422, (length-normalized): 0.91485, angle RMSD: 79.973\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0034 Batch 000/087: Loss: 15.268, dist RMSD: 12.06, (length-normalized): 0.97374, angle RMSD: 80.214\n",
            "Epoch 0034 Batch 010/087: Loss: 14.158, dist RMSD: 10.981, (length-normalized): 0.88519, angle RMSD: 79.43\n",
            "Epoch 0034 Batch 020/087: Loss: 13.338, dist RMSD: 10.055, (length-normalized): 0.90816, angle RMSD: 82.078\n",
            "Epoch 0034 Batch 030/087: Loss: 13.911, dist RMSD: 10.525, (length-normalized): 0.97878, angle RMSD: 84.657\n",
            "Epoch 0034 Batch 040/087: Loss: 13.078, dist RMSD: 10.022, (length-normalized): 0.9237, angle RMSD: 76.402\n",
            "Epoch 0034 Batch 050/087: Loss: 13.244, dist RMSD: 9.9734, (length-normalized): 0.94649, angle RMSD: 81.768\n",
            "Epoch 0034 Batch 060/087: Loss: 12.678, dist RMSD: 9.4211, (length-normalized): 0.82073, angle RMSD: 81.432\n",
            "Epoch 0034 Batch 070/087: Loss: 13.62, dist RMSD: 10.36, (length-normalized): 0.8997, angle RMSD: 81.51\n",
            "Epoch 0034 Batch 080/087: Loss: 12.982, dist RMSD: 9.8299, (length-normalized): 0.86308, angle RMSD: 78.813\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 034: Train Loss: 13.501, dist RMSD: 10.29, (length-normalized): 0.91103, angle RMSD: 80.266 \n",
            "              Val Loss: 13.757, dist RMSD: 10.562, (length-normalized): 0.91712, angle RMSD: 79.861\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0035 Batch 000/087: Loss: 14.064, dist RMSD: 10.777, (length-normalized): 0.97104, angle RMSD: 82.195\n",
            "Epoch 0035 Batch 010/087: Loss: 13.095, dist RMSD: 9.8219, (length-normalized): 0.82532, angle RMSD: 81.831\n",
            "Epoch 0035 Batch 020/087: Loss: 12.198, dist RMSD: 8.9254, (length-normalized): 0.87933, angle RMSD: 81.809\n",
            "Epoch 0035 Batch 030/087: Loss: 12.794, dist RMSD: 9.6634, (length-normalized): 0.91793, angle RMSD: 78.276\n",
            "Epoch 0035 Batch 040/087: Loss: 13.498, dist RMSD: 10.364, (length-normalized): 0.83825, angle RMSD: 78.333\n",
            "Epoch 0035 Batch 050/087: Loss: 12.423, dist RMSD: 9.2707, (length-normalized): 0.78861, angle RMSD: 78.806\n",
            "Epoch 0035 Batch 060/087: Loss: 13.037, dist RMSD: 9.8678, (length-normalized): 0.85625, angle RMSD: 79.225\n",
            "Epoch 0035 Batch 070/087: Loss: 12.698, dist RMSD: 9.5044, (length-normalized): 0.83901, angle RMSD: 79.852\n",
            "Epoch 0035 Batch 080/087: Loss: 13.151, dist RMSD: 9.9866, (length-normalized): 0.90942, angle RMSD: 79.121\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 035: Train Loss: 13.428, dist RMSD: 10.224, (length-normalized): 0.9051, angle RMSD: 80.096 \n",
            "              Val Loss: 13.632, dist RMSD: 10.441, (length-normalized): 0.90922, angle RMSD: 79.755\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0036 Batch 000/087: Loss: 12.488, dist RMSD: 9.339, (length-normalized): 0.85004, angle RMSD: 78.732\n",
            "Epoch 0036 Batch 010/087: Loss: 12.398, dist RMSD: 9.2895, (length-normalized): 0.84757, angle RMSD: 77.702\n",
            "Epoch 0036 Batch 020/087: Loss: 13.075, dist RMSD: 9.7901, (length-normalized): 0.87162, angle RMSD: 82.127\n",
            "Epoch 0036 Batch 030/087: Loss: 13.243, dist RMSD: 9.964, (length-normalized): 0.85361, angle RMSD: 81.969\n",
            "Epoch 0036 Batch 040/087: Loss: 13.329, dist RMSD: 10.153, (length-normalized): 0.90049, angle RMSD: 79.419\n",
            "Epoch 0036 Batch 050/087: Loss: 12.758, dist RMSD: 9.5629, (length-normalized): 0.89518, angle RMSD: 79.87\n",
            "Epoch 0036 Batch 060/087: Loss: 12.84, dist RMSD: 9.5044, (length-normalized): 0.89143, angle RMSD: 83.378\n",
            "Epoch 0036 Batch 070/087: Loss: 12.803, dist RMSD: 9.6565, (length-normalized): 0.828, angle RMSD: 78.67\n",
            "Epoch 0036 Batch 080/087: Loss: 14.185, dist RMSD: 11.027, (length-normalized): 0.91813, angle RMSD: 78.939\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 036: Train Loss: 13.317, dist RMSD: 10.119, (length-normalized): 0.89426, angle RMSD: 79.946 \n",
            "              Val Loss: 13.566, dist RMSD: 10.383, (length-normalized): 0.90321, angle RMSD: 79.567\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0037 Batch 000/087: Loss: 12.723, dist RMSD: 9.4824, (length-normalized): 0.94565, angle RMSD: 81.004\n",
            "Epoch 0037 Batch 010/087: Loss: 11.964, dist RMSD: 8.6228, (length-normalized): 0.8042, angle RMSD: 83.526\n",
            "Epoch 0037 Batch 020/087: Loss: 14.392, dist RMSD: 11.388, (length-normalized): 0.98067, angle RMSD: 75.12\n",
            "Epoch 0037 Batch 030/087: Loss: 13.842, dist RMSD: 10.75, (length-normalized): 0.94309, angle RMSD: 77.287\n",
            "Epoch 0037 Batch 040/087: Loss: 13.857, dist RMSD: 10.481, (length-normalized): 0.87205, angle RMSD: 84.397\n",
            "Epoch 0037 Batch 050/087: Loss: 14.278, dist RMSD: 10.951, (length-normalized): 0.97898, angle RMSD: 83.169\n",
            "Epoch 0037 Batch 060/087: Loss: 13.323, dist RMSD: 10.248, (length-normalized): 0.82758, angle RMSD: 76.877\n",
            "Epoch 0037 Batch 070/087: Loss: 12.134, dist RMSD: 8.898, (length-normalized): 0.82256, angle RMSD: 80.906\n",
            "Epoch 0037 Batch 080/087: Loss: 14.456, dist RMSD: 11.31, (length-normalized): 0.97463, angle RMSD: 78.641\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 037: Train Loss: 13.23, dist RMSD: 10.038, (length-normalized): 0.88534, angle RMSD: 79.8 \n",
            "              Val Loss: 13.555, dist RMSD: 10.372, (length-normalized): 0.9012, angle RMSD: 79.572\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0038 Batch 000/087: Loss: 14.839, dist RMSD: 11.664, (length-normalized): 0.99902, angle RMSD: 79.371\n",
            "Epoch 0038 Batch 010/087: Loss: 12.815, dist RMSD: 9.632, (length-normalized): 0.84447, angle RMSD: 79.562\n",
            "Epoch 0038 Batch 020/087: Loss: 12.19, dist RMSD: 8.7763, (length-normalized): 0.83126, angle RMSD: 85.344\n",
            "Epoch 0038 Batch 030/087: Loss: 13.444, dist RMSD: 10.179, (length-normalized): 0.83689, angle RMSD: 81.62\n",
            "Epoch 0038 Batch 040/087: Loss: 12.666, dist RMSD: 9.6057, (length-normalized): 0.83606, angle RMSD: 76.514\n",
            "Epoch 0038 Batch 050/087: Loss: 13.933, dist RMSD: 10.654, (length-normalized): 0.90924, angle RMSD: 81.959\n",
            "Epoch 0038 Batch 060/087: Loss: 13.411, dist RMSD: 10.069, (length-normalized): 0.97544, angle RMSD: 83.533\n",
            "Epoch 0038 Batch 070/087: Loss: 12.259, dist RMSD: 9.0603, (length-normalized): 0.87997, angle RMSD: 79.97\n",
            "Epoch 0038 Batch 080/087: Loss: 12.406, dist RMSD: 9.1437, (length-normalized): 0.86715, angle RMSD: 81.55\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 038: Train Loss: 13.155, dist RMSD: 9.9685, (length-normalized): 0.8772, angle RMSD: 79.668 \n",
            "              Val Loss: 13.526, dist RMSD: 10.351, (length-normalized): 0.90255, angle RMSD: 79.376\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0039 Batch 000/087: Loss: 11.16, dist RMSD: 8.0969, (length-normalized): 0.7618, angle RMSD: 76.567\n",
            "Epoch 0039 Batch 010/087: Loss: 12.014, dist RMSD: 8.8683, (length-normalized): 0.79608, angle RMSD: 78.642\n",
            "Epoch 0039 Batch 020/087: Loss: 13.012, dist RMSD: 9.8762, (length-normalized): 0.88949, angle RMSD: 78.383\n",
            "Epoch 0039 Batch 030/087: Loss: 12.875, dist RMSD: 9.8092, (length-normalized): 0.81759, angle RMSD: 76.642\n",
            "Epoch 0039 Batch 040/087: Loss: 12.86, dist RMSD: 9.6882, (length-normalized): 0.83335, angle RMSD: 79.295\n",
            "Epoch 0039 Batch 050/087: Loss: 12.605, dist RMSD: 9.4509, (length-normalized): 0.82859, angle RMSD: 78.856\n",
            "Epoch 0039 Batch 060/087: Loss: 12.403, dist RMSD: 9.3157, (length-normalized): 0.83139, angle RMSD: 77.176\n",
            "Epoch 0039 Batch 070/087: Loss: 12.9, dist RMSD: 9.6888, (length-normalized): 0.92296, angle RMSD: 80.281\n",
            "Epoch 0039 Batch 080/087: Loss: 12.707, dist RMSD: 9.5526, (length-normalized): 0.82764, angle RMSD: 78.871\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 039: Train Loss: 13.028, dist RMSD: 9.8479, (length-normalized): 0.86528, angle RMSD: 79.501 \n",
            "              Val Loss: 13.401, dist RMSD: 10.241, (length-normalized): 0.89294, angle RMSD: 78.998\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0040 Batch 000/087: Loss: 13.265, dist RMSD: 10.077, (length-normalized): 0.83444, angle RMSD: 79.708\n",
            "Epoch 0040 Batch 010/087: Loss: 13.776, dist RMSD: 10.641, (length-normalized): 0.92707, angle RMSD: 78.369\n",
            "Epoch 0040 Batch 020/087: Loss: 12.623, dist RMSD: 9.4526, (length-normalized): 0.85855, angle RMSD: 79.25\n",
            "Epoch 0040 Batch 030/087: Loss: 14.118, dist RMSD: 11.017, (length-normalized): 0.88198, angle RMSD: 77.52\n",
            "Epoch 0040 Batch 040/087: Loss: 13.494, dist RMSD: 10.274, (length-normalized): 0.93354, angle RMSD: 80.497\n",
            "Epoch 0040 Batch 050/087: Loss: 12.44, dist RMSD: 9.1254, (length-normalized): 0.81894, angle RMSD: 82.877\n",
            "Epoch 0040 Batch 060/087: Loss: 12.346, dist RMSD: 9.0734, (length-normalized): 0.86325, angle RMSD: 81.814\n",
            "Epoch 0040 Batch 070/087: Loss: 13.388, dist RMSD: 10.054, (length-normalized): 0.92867, angle RMSD: 83.351\n",
            "Epoch 0040 Batch 080/087: Loss: 13.5, dist RMSD: 10.451, (length-normalized): 0.84421, angle RMSD: 76.221\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 040: Train Loss: 13.058, dist RMSD: 9.8812, (length-normalized): 0.86804, angle RMSD: 79.429 \n",
            "              Val Loss: 14.624, dist RMSD: 11.463, (length-normalized): 0.98883, angle RMSD: 79.025\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0041 Batch 000/087: Loss: 14.545, dist RMSD: 11.378, (length-normalized): 0.91964, angle RMSD: 79.178\n",
            "Epoch 0041 Batch 010/087: Loss: 13.448, dist RMSD: 10.295, (length-normalized): 0.93182, angle RMSD: 78.831\n",
            "Epoch 0041 Batch 020/087: Loss: 12.387, dist RMSD: 9.1857, (length-normalized): 0.80762, angle RMSD: 80.026\n",
            "Epoch 0041 Batch 030/087: Loss: 12.607, dist RMSD: 9.4285, (length-normalized): 0.85565, angle RMSD: 79.463\n",
            "Epoch 0041 Batch 040/087: Loss: 15.47, dist RMSD: 12.291, (length-normalized): 1.207, angle RMSD: 79.472\n",
            "Epoch 0041 Batch 050/087: Loss: 12.167, dist RMSD: 9.0903, (length-normalized): 0.77783, angle RMSD: 76.909\n",
            "Epoch 0041 Batch 060/087: Loss: 12.305, dist RMSD: 9.231, (length-normalized): 0.8114, angle RMSD: 76.841\n",
            "Epoch 0041 Batch 070/087: Loss: 12.498, dist RMSD: 9.4817, (length-normalized): 0.79458, angle RMSD: 75.397\n",
            "Epoch 0041 Batch 080/087: Loss: 13.39, dist RMSD: 10.073, (length-normalized): 0.9763, angle RMSD: 82.927\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 041: Train Loss: 12.971, dist RMSD: 9.802, (length-normalized): 0.86079, angle RMSD: 79.233 \n",
            "              Val Loss: 13.206, dist RMSD: 10.042, (length-normalized): 0.86957, angle RMSD: 79.103\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0042 Batch 000/087: Loss: 12.989, dist RMSD: 9.885, (length-normalized): 0.84572, angle RMSD: 77.601\n",
            "Epoch 0042 Batch 010/087: Loss: 13.134, dist RMSD: 9.9419, (length-normalized): 0.82256, angle RMSD: 79.802\n",
            "Epoch 0042 Batch 020/087: Loss: 13.398, dist RMSD: 10.149, (length-normalized): 0.90719, angle RMSD: 81.226\n",
            "Epoch 0042 Batch 030/087: Loss: 13.284, dist RMSD: 9.9933, (length-normalized): 0.91606, angle RMSD: 82.27\n",
            "Epoch 0042 Batch 040/087: Loss: 13.612, dist RMSD: 10.291, (length-normalized): 0.8795, angle RMSD: 83.025\n",
            "Epoch 0042 Batch 050/087: Loss: 13.528, dist RMSD: 10.341, (length-normalized): 0.85303, angle RMSD: 79.67\n",
            "Epoch 0042 Batch 060/087: Loss: 13.134, dist RMSD: 10.017, (length-normalized): 0.82298, angle RMSD: 77.944\n",
            "Epoch 0042 Batch 070/087: Loss: 15.006, dist RMSD: 12.005, (length-normalized): 0.9263, angle RMSD: 75.028\n",
            "Epoch 0042 Batch 080/087: Loss: 13.084, dist RMSD: 9.9811, (length-normalized): 0.79331, angle RMSD: 77.561\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 042: Train Loss: 12.919, dist RMSD: 9.7498, (length-normalized): 0.8564, angle RMSD: 79.22 \n",
            "              Val Loss: 13.229, dist RMSD: 10.071, (length-normalized): 0.87188, angle RMSD: 78.941\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0043 Batch 000/087: Loss: 12.049, dist RMSD: 8.9241, (length-normalized): 0.78522, angle RMSD: 78.134\n",
            "Epoch 0043 Batch 010/087: Loss: 12.744, dist RMSD: 9.5837, (length-normalized): 0.7914, angle RMSD: 79.016\n",
            "Epoch 0043 Batch 020/087: Loss: 12.903, dist RMSD: 9.6486, (length-normalized): 0.79428, angle RMSD: 81.355\n",
            "Epoch 0043 Batch 030/087: Loss: 11.77, dist RMSD: 8.7173, (length-normalized): 0.75279, angle RMSD: 76.309\n",
            "Epoch 0043 Batch 040/087: Loss: 13.893, dist RMSD: 10.564, (length-normalized): 0.95377, angle RMSD: 83.227\n",
            "Epoch 0043 Batch 050/087: Loss: 13.736, dist RMSD: 10.441, (length-normalized): 0.93513, angle RMSD: 82.374\n",
            "Epoch 0043 Batch 060/087: Loss: 11.997, dist RMSD: 8.9479, (length-normalized): 0.79203, angle RMSD: 76.222\n",
            "Epoch 0043 Batch 070/087: Loss: 13.124, dist RMSD: 10.033, (length-normalized): 0.81161, angle RMSD: 77.266\n",
            "Epoch 0043 Batch 080/087: Loss: 12.982, dist RMSD: 9.778, (length-normalized): 0.87231, angle RMSD: 80.088\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 043: Train Loss: 12.861, dist RMSD: 9.6953, (length-normalized): 0.84872, angle RMSD: 79.146 \n",
            "              Val Loss: 13.234, dist RMSD: 10.086, (length-normalized): 0.87804, angle RMSD: 78.691\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0044 Batch 000/087: Loss: 13.184, dist RMSD: 10.104, (length-normalized): 0.87449, angle RMSD: 76.993\n",
            "Epoch 0044 Batch 010/087: Loss: 14.94, dist RMSD: 11.763, (length-normalized): 1.0629, angle RMSD: 79.428\n",
            "Epoch 0044 Batch 020/087: Loss: 12.464, dist RMSD: 9.2872, (length-normalized): 0.77061, angle RMSD: 79.42\n",
            "Epoch 0044 Batch 030/087: Loss: 12.101, dist RMSD: 8.9894, (length-normalized): 0.77489, angle RMSD: 77.788\n",
            "Epoch 0044 Batch 040/087: Loss: 12.511, dist RMSD: 9.408, (length-normalized): 0.78632, angle RMSD: 77.585\n",
            "Epoch 0044 Batch 050/087: Loss: 13.685, dist RMSD: 10.518, (length-normalized): 0.87967, angle RMSD: 79.179\n",
            "Epoch 0044 Batch 060/087: Loss: 12.528, dist RMSD: 9.3224, (length-normalized): 0.85383, angle RMSD: 80.135\n",
            "Epoch 0044 Batch 070/087: Loss: 11.56, dist RMSD: 8.4905, (length-normalized): 0.76633, angle RMSD: 76.738\n",
            "Epoch 0044 Batch 080/087: Loss: 12.169, dist RMSD: 9.0294, (length-normalized): 0.82583, angle RMSD: 78.5\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 044: Train Loss: 12.831, dist RMSD: 9.6725, (length-normalized): 0.84714, angle RMSD: 78.974 \n",
            "              Val Loss: 13.413, dist RMSD: 10.261, (length-normalized): 0.88359, angle RMSD: 78.802\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0045 Batch 000/087: Loss: 13.176, dist RMSD: 9.9445, (length-normalized): 0.89308, angle RMSD: 80.779\n",
            "Epoch 0045 Batch 010/087: Loss: 11.951, dist RMSD: 8.6674, (length-normalized): 0.83999, angle RMSD: 82.093\n",
            "Epoch 0045 Batch 020/087: Loss: 13.086, dist RMSD: 9.6314, (length-normalized): 0.91622, angle RMSD: 86.359\n",
            "Epoch 0045 Batch 030/087: Loss: 12.12, dist RMSD: 9.0027, (length-normalized): 0.86356, angle RMSD: 77.927\n",
            "Epoch 0045 Batch 040/087: Loss: 13.118, dist RMSD: 10.054, (length-normalized): 0.85766, angle RMSD: 76.6\n",
            "Epoch 0045 Batch 050/087: Loss: 12.442, dist RMSD: 9.3228, (length-normalized): 0.8021, angle RMSD: 77.992\n",
            "Epoch 0045 Batch 060/087: Loss: 12.813, dist RMSD: 9.595, (length-normalized): 0.84956, angle RMSD: 80.459\n",
            "Epoch 0045 Batch 070/087: Loss: 11.899, dist RMSD: 8.7855, (length-normalized): 0.80898, angle RMSD: 77.831\n",
            "Epoch 0045 Batch 080/087: Loss: 12.558, dist RMSD: 9.436, (length-normalized): 0.83551, angle RMSD: 78.04\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 045: Train Loss: 12.826, dist RMSD: 9.6666, (length-normalized): 0.84745, angle RMSD: 78.996 \n",
            "              Val Loss: 13.061, dist RMSD: 9.9149, (length-normalized): 0.86277, angle RMSD: 78.655\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0046 Batch 000/087: Loss: 13.936, dist RMSD: 10.725, (length-normalized): 0.91085, angle RMSD: 80.28\n",
            "Epoch 0046 Batch 010/087: Loss: 12.153, dist RMSD: 8.7652, (length-normalized): 0.8124, angle RMSD: 84.695\n",
            "Epoch 0046 Batch 020/087: Loss: 12.135, dist RMSD: 9.1184, (length-normalized): 0.80988, angle RMSD: 75.421\n",
            "Epoch 0046 Batch 030/087: Loss: 12.506, dist RMSD: 9.2822, (length-normalized): 0.84879, angle RMSD: 80.591\n",
            "Epoch 0046 Batch 040/087: Loss: 11.622, dist RMSD: 8.5785, (length-normalized): 0.76158, angle RMSD: 76.083\n",
            "Epoch 0046 Batch 050/087: Loss: 12.432, dist RMSD: 9.2895, (length-normalized): 0.83829, angle RMSD: 78.566\n",
            "Epoch 0046 Batch 060/087: Loss: 13.636, dist RMSD: 10.475, (length-normalized): 0.8826, angle RMSD: 79.041\n",
            "Epoch 0046 Batch 070/087: Loss: 12.815, dist RMSD: 9.6215, (length-normalized): 0.85175, angle RMSD: 79.847\n",
            "Epoch 0046 Batch 080/087: Loss: 11.667, dist RMSD: 8.5811, (length-normalized): 0.75449, angle RMSD: 77.147\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 046: Train Loss: 12.737, dist RMSD: 9.573, (length-normalized): 0.83857, angle RMSD: 79.089 \n",
            "              Val Loss: 13.089, dist RMSD: 9.9496, (length-normalized): 0.86186, angle RMSD: 78.477\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0047 Batch 000/087: Loss: 13.083, dist RMSD: 9.9035, (length-normalized): 0.78475, angle RMSD: 79.5\n",
            "Epoch 0047 Batch 010/087: Loss: 11.439, dist RMSD: 8.305, (length-normalized): 0.78172, angle RMSD: 78.361\n",
            "Epoch 0047 Batch 020/087: Loss: 11.076, dist RMSD: 8.0092, (length-normalized): 0.71711, angle RMSD: 76.662\n",
            "Epoch 0047 Batch 030/087: Loss: 12.985, dist RMSD: 9.7402, (length-normalized): 0.85464, angle RMSD: 81.129\n",
            "Epoch 0047 Batch 040/087: Loss: 12.711, dist RMSD: 9.5087, (length-normalized): 0.88278, angle RMSD: 80.052\n",
            "Epoch 0047 Batch 050/087: Loss: 13.461, dist RMSD: 10.106, (length-normalized): 0.94693, angle RMSD: 83.873\n",
            "Epoch 0047 Batch 060/087: Loss: 14.271, dist RMSD: 11.063, (length-normalized): 1.0106, angle RMSD: 80.2\n",
            "Epoch 0047 Batch 070/087: Loss: 13.184, dist RMSD: 10.034, (length-normalized): 0.83314, angle RMSD: 78.744\n",
            "Epoch 0047 Batch 080/087: Loss: 12.315, dist RMSD: 9.2358, (length-normalized): 0.76756, angle RMSD: 76.978\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 047: Train Loss: 12.703, dist RMSD: 9.5518, (length-normalized): 0.83648, angle RMSD: 78.77 \n",
            "              Val Loss: 13.128, dist RMSD: 9.9953, (length-normalized): 0.86131, angle RMSD: 78.33\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0048 Batch 000/087: Loss: 11.69, dist RMSD: 8.5967, (length-normalized): 0.77058, angle RMSD: 77.33\n",
            "Epoch 0048 Batch 010/087: Loss: 12.556, dist RMSD: 9.4484, (length-normalized): 0.84696, angle RMSD: 77.701\n",
            "Epoch 0048 Batch 020/087: Loss: 14.34, dist RMSD: 11.294, (length-normalized): 0.87742, angle RMSD: 76.154\n",
            "Epoch 0048 Batch 030/087: Loss: 12.324, dist RMSD: 9.0308, (length-normalized): 0.84877, angle RMSD: 82.329\n",
            "Epoch 0048 Batch 040/087: Loss: 14.751, dist RMSD: 11.407, (length-normalized): 0.98149, angle RMSD: 83.594\n",
            "Epoch 0048 Batch 050/087: Loss: 12.199, dist RMSD: 9.0123, (length-normalized): 0.77258, angle RMSD: 79.674\n",
            "Epoch 0048 Batch 060/087: Loss: 12.651, dist RMSD: 9.5085, (length-normalized): 0.81201, angle RMSD: 78.565\n",
            "Epoch 0048 Batch 070/087: Loss: 11.84, dist RMSD: 8.5922, (length-normalized): 0.76796, angle RMSD: 81.19\n",
            "Epoch 0048 Batch 080/087: Loss: 12.127, dist RMSD: 9.1028, (length-normalized): 0.785, angle RMSD: 75.612\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 048: Train Loss: 12.676, dist RMSD: 9.5309, (length-normalized): 0.83461, angle RMSD: 78.62 \n",
            "              Val Loss: 13.24, dist RMSD: 10.108, (length-normalized): 0.88555, angle RMSD: 78.294\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0049 Batch 000/087: Loss: 12.583, dist RMSD: 9.4835, (length-normalized): 0.79578, angle RMSD: 77.492\n",
            "Epoch 0049 Batch 010/087: Loss: 11.027, dist RMSD: 7.9628, (length-normalized): 0.75247, angle RMSD: 76.595\n",
            "Epoch 0049 Batch 020/087: Loss: 13.35, dist RMSD: 10.348, (length-normalized): 0.84824, angle RMSD: 75.062\n",
            "Epoch 0049 Batch 030/087: Loss: 11.982, dist RMSD: 8.7135, (length-normalized): 0.80716, angle RMSD: 81.717\n",
            "Epoch 0049 Batch 040/087: Loss: 13.437, dist RMSD: 10.263, (length-normalized): 0.85997, angle RMSD: 79.36\n",
            "Epoch 0049 Batch 050/087: Loss: 11.378, dist RMSD: 8.3796, (length-normalized): 0.75372, angle RMSD: 74.973\n",
            "Epoch 0049 Batch 060/087: Loss: 14.309, dist RMSD: 11.141, (length-normalized): 0.89252, angle RMSD: 79.196\n",
            "Epoch 0049 Batch 070/087: Loss: 11.257, dist RMSD: 8.2043, (length-normalized): 0.76205, angle RMSD: 76.328\n",
            "Epoch 0049 Batch 080/087: Loss: 12.081, dist RMSD: 8.9377, (length-normalized): 0.76617, angle RMSD: 78.574\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 049: Train Loss: 12.627, dist RMSD: 9.4841, (length-normalized): 0.83086, angle RMSD: 78.563 \n",
            "              Val Loss: 13.742, dist RMSD: 10.613, (length-normalized): 0.90164, angle RMSD: 78.23\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0050 Batch 000/087: Loss: 12.849, dist RMSD: 9.7673, (length-normalized): 0.86108, angle RMSD: 77.048\n",
            "Epoch 0050 Batch 010/087: Loss: 12.39, dist RMSD: 9.1683, (length-normalized): 0.83452, angle RMSD: 80.541\n",
            "Epoch 0050 Batch 020/087: Loss: 12.751, dist RMSD: 9.5636, (length-normalized): 0.83283, angle RMSD: 79.692\n",
            "Epoch 0050 Batch 030/087: Loss: 13.777, dist RMSD: 10.54, (length-normalized): 0.95637, angle RMSD: 80.943\n",
            "Epoch 0050 Batch 040/087: Loss: 12.78, dist RMSD: 9.6933, (length-normalized): 0.77671, angle RMSD: 77.163\n",
            "Epoch 0050 Batch 050/087: Loss: 12.387, dist RMSD: 9.1655, (length-normalized): 0.73459, angle RMSD: 80.543\n",
            "Epoch 0050 Batch 060/087: Loss: 12.074, dist RMSD: 8.836, (length-normalized): 0.83861, angle RMSD: 80.962\n",
            "Epoch 0050 Batch 070/087: Loss: 12.193, dist RMSD: 8.9894, (length-normalized): 0.7871, angle RMSD: 80.1\n",
            "Epoch 0050 Batch 080/087: Loss: 13.043, dist RMSD: 9.9735, (length-normalized): 0.87507, angle RMSD: 76.73\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 050: Train Loss: 12.606, dist RMSD: 9.4705, (length-normalized): 0.83, angle RMSD: 78.387 \n",
            "              Val Loss: 12.94, dist RMSD: 9.8232, (length-normalized): 0.85246, angle RMSD: 77.933\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0051 Batch 000/087: Loss: 13.146, dist RMSD: 10.177, (length-normalized): 0.91019, angle RMSD: 74.223\n",
            "Epoch 0051 Batch 010/087: Loss: 12.985, dist RMSD: 9.8779, (length-normalized): 0.94688, angle RMSD: 77.686\n",
            "Epoch 0051 Batch 020/087: Loss: 12.269, dist RMSD: 9.1187, (length-normalized): 0.83982, angle RMSD: 78.746\n",
            "Epoch 0051 Batch 030/087: Loss: 12.4, dist RMSD: 9.2692, (length-normalized): 0.82055, angle RMSD: 78.261\n",
            "Epoch 0051 Batch 040/087: Loss: 13.548, dist RMSD: 10.47, (length-normalized): 0.86427, angle RMSD: 76.948\n",
            "Epoch 0051 Batch 050/087: Loss: 12.049, dist RMSD: 9.0341, (length-normalized): 0.81793, angle RMSD: 75.362\n",
            "Epoch 0051 Batch 060/087: Loss: 13.463, dist RMSD: 10.205, (length-normalized): 0.84495, angle RMSD: 81.455\n",
            "Epoch 0051 Batch 070/087: Loss: 13.058, dist RMSD: 9.9543, (length-normalized): 0.7775, angle RMSD: 77.59\n",
            "Epoch 0051 Batch 080/087: Loss: 14.323, dist RMSD: 11.272, (length-normalized): 0.97409, angle RMSD: 76.265\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 051: Train Loss: 12.548, dist RMSD: 9.4201, (length-normalized): 0.82441, angle RMSD: 78.196 \n",
            "              Val Loss: 13.146, dist RMSD: 10.037, (length-normalized): 0.86864, angle RMSD: 77.727\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0052 Batch 000/087: Loss: 12.592, dist RMSD: 9.4067, (length-normalized): 0.78663, angle RMSD: 79.633\n",
            "Epoch 0052 Batch 010/087: Loss: 13.145, dist RMSD: 10.051, (length-normalized): 0.83764, angle RMSD: 77.354\n",
            "Epoch 0052 Batch 020/087: Loss: 13.11, dist RMSD: 10.081, (length-normalized): 0.90903, angle RMSD: 75.723\n",
            "Epoch 0052 Batch 030/087: Loss: 12.489, dist RMSD: 9.3045, (length-normalized): 0.83302, angle RMSD: 79.617\n",
            "Epoch 0052 Batch 040/087: Loss: 11.808, dist RMSD: 8.7318, (length-normalized): 0.78069, angle RMSD: 76.91\n",
            "Epoch 0052 Batch 050/087: Loss: 14.447, dist RMSD: 11.062, (length-normalized): 0.92725, angle RMSD: 84.627\n",
            "Epoch 0052 Batch 060/087: Loss: 11.775, dist RMSD: 8.6156, (length-normalized): 0.80616, angle RMSD: 78.978\n",
            "Epoch 0052 Batch 070/087: Loss: 12.225, dist RMSD: 9.1545, (length-normalized): 0.8224, angle RMSD: 76.752\n",
            "Epoch 0052 Batch 080/087: Loss: 12.298, dist RMSD: 8.9623, (length-normalized): 0.83858, angle RMSD: 83.386\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 052: Train Loss: 12.512, dist RMSD: 9.394, (length-normalized): 0.82364, angle RMSD: 77.961 \n",
            "              Val Loss: 12.953, dist RMSD: 9.8571, (length-normalized): 0.85273, angle RMSD: 77.391\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0053 Batch 000/087: Loss: 12.47, dist RMSD: 9.2179, (length-normalized): 0.84266, angle RMSD: 81.305\n",
            "Epoch 0053 Batch 010/087: Loss: 12.829, dist RMSD: 9.6823, (length-normalized): 0.87694, angle RMSD: 78.673\n",
            "Epoch 0053 Batch 020/087: Loss: 12.205, dist RMSD: 9.0379, (length-normalized): 0.81373, angle RMSD: 79.19\n",
            "Epoch 0053 Batch 030/087: Loss: 12.839, dist RMSD: 9.7363, (length-normalized): 0.89821, angle RMSD: 77.556\n",
            "Epoch 0053 Batch 040/087: Loss: 11.246, dist RMSD: 8.1368, (length-normalized): 0.7591, angle RMSD: 77.738\n",
            "Epoch 0053 Batch 050/087: Loss: 11.685, dist RMSD: 8.5591, (length-normalized): 0.77751, angle RMSD: 78.145\n",
            "Epoch 0053 Batch 060/087: Loss: 12.115, dist RMSD: 9.062, (length-normalized): 0.78905, angle RMSD: 76.337\n",
            "Epoch 0053 Batch 070/087: Loss: 14.428, dist RMSD: 11.333, (length-normalized): 0.95961, angle RMSD: 77.376\n",
            "Epoch 0053 Batch 080/087: Loss: 12.328, dist RMSD: 9.1703, (length-normalized): 0.7948, angle RMSD: 78.938\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 053: Train Loss: 12.465, dist RMSD: 9.3571, (length-normalized): 0.8197, angle RMSD: 77.696 \n",
            "              Val Loss: 12.889, dist RMSD: 9.7833, (length-normalized): 0.84766, angle RMSD: 77.651\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0054 Batch 000/087: Loss: 11.48, dist RMSD: 8.3978, (length-normalized): 0.79866, angle RMSD: 77.059\n",
            "Epoch 0054 Batch 010/087: Loss: 14.914, dist RMSD: 11.92, (length-normalized): 0.99979, angle RMSD: 74.843\n",
            "Epoch 0054 Batch 020/087: Loss: 11.891, dist RMSD: 8.8591, (length-normalized): 0.80132, angle RMSD: 75.792\n",
            "Epoch 0054 Batch 030/087: Loss: 11.516, dist RMSD: 8.5933, (length-normalized): 0.72396, angle RMSD: 73.068\n",
            "Epoch 0054 Batch 040/087: Loss: 12.576, dist RMSD: 9.5378, (length-normalized): 0.82527, angle RMSD: 75.952\n",
            "Epoch 0054 Batch 050/087: Loss: 12.589, dist RMSD: 9.2399, (length-normalized): 0.80928, angle RMSD: 83.735\n",
            "Epoch 0054 Batch 060/087: Loss: 13.495, dist RMSD: 10.36, (length-normalized): 0.84035, angle RMSD: 78.379\n",
            "Epoch 0054 Batch 070/087: Loss: 11.471, dist RMSD: 8.3543, (length-normalized): 0.7868, angle RMSD: 77.906\n",
            "Epoch 0054 Batch 080/087: Loss: 11.866, dist RMSD: 8.8721, (length-normalized): 0.75694, angle RMSD: 74.851\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 054: Train Loss: 12.469, dist RMSD: 9.3459, (length-normalized): 0.81923, angle RMSD: 78.082 \n",
            "              Val Loss: 13.179, dist RMSD: 10.02, (length-normalized): 0.85889, angle RMSD: 78.971\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0055 Batch 000/087: Loss: 11.755, dist RMSD: 8.8737, (length-normalized): 0.73114, angle RMSD: 72.021\n",
            "Epoch 0055 Batch 010/087: Loss: 11.209, dist RMSD: 7.9349, (length-normalized): 0.6917, angle RMSD: 81.852\n",
            "Epoch 0055 Batch 020/087: Loss: 11.3, dist RMSD: 8.2425, (length-normalized): 0.80696, angle RMSD: 76.44\n",
            "Epoch 0055 Batch 030/087: Loss: 13.041, dist RMSD: 9.7188, (length-normalized): 0.88095, angle RMSD: 83.055\n",
            "Epoch 0055 Batch 040/087: Loss: 11.758, dist RMSD: 8.4229, (length-normalized): 0.87341, angle RMSD: 83.387\n",
            "Epoch 0055 Batch 050/087: Loss: 12.855, dist RMSD: 9.5842, (length-normalized): 0.78643, angle RMSD: 81.761\n",
            "Epoch 0055 Batch 060/087: Loss: 13.335, dist RMSD: 9.9761, (length-normalized): 0.79268, angle RMSD: 83.973\n",
            "Epoch 0055 Batch 070/087: Loss: 13.131, dist RMSD: 9.6149, (length-normalized): 0.86326, angle RMSD: 87.89\n",
            "Epoch 0055 Batch 080/087: Loss: 13.674, dist RMSD: 10.139, (length-normalized): 0.9014, angle RMSD: 88.375\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 055: Train Loss: 12.616, dist RMSD: 9.3505, (length-normalized): 0.81878, angle RMSD: 81.643 \n",
            "              Val Loss: 13.043, dist RMSD: 9.6847, (length-normalized): 0.84059, angle RMSD: 83.95\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0056 Batch 000/087: Loss: 12.009, dist RMSD: 8.7059, (length-normalized): 0.80991, angle RMSD: 82.586\n",
            "Epoch 0056 Batch 010/087: Loss: 11.755, dist RMSD: 8.4638, (length-normalized): 0.75695, angle RMSD: 82.267\n",
            "Epoch 0056 Batch 020/087: Loss: 12.246, dist RMSD: 8.9787, (length-normalized): 0.75175, angle RMSD: 81.68\n",
            "Epoch 0056 Batch 030/087: Loss: 12.263, dist RMSD: 9.0335, (length-normalized): 0.77924, angle RMSD: 80.745\n",
            "Epoch 0056 Batch 040/087: Loss: 11.236, dist RMSD: 7.6934, (length-normalized): 0.73088, angle RMSD: 88.573\n",
            "Epoch 0056 Batch 050/087: Loss: 12.229, dist RMSD: 8.7881, (length-normalized): 0.82331, angle RMSD: 86.027\n",
            "Epoch 0056 Batch 060/087: Loss: 13.263, dist RMSD: 9.9529, (length-normalized): 0.92585, angle RMSD: 82.747\n",
            "Epoch 0056 Batch 070/087: Loss: 12.37, dist RMSD: 8.973, (length-normalized): 0.76976, angle RMSD: 84.93\n",
            "Epoch 0056 Batch 080/087: Loss: 14.962, dist RMSD: 11.527, (length-normalized): 0.95443, angle RMSD: 85.876\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 056: Train Loss: 12.697, dist RMSD: 9.3123, (length-normalized): 0.8148, angle RMSD: 84.608 \n",
            "              Val Loss: 13.361, dist RMSD: 9.9879, (length-normalized): 0.86482, angle RMSD: 84.316\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0057 Batch 000/087: Loss: 13.148, dist RMSD: 9.8478, (length-normalized): 0.83113, angle RMSD: 82.504\n",
            "Epoch 0057 Batch 010/087: Loss: 12.801, dist RMSD: 9.4722, (length-normalized): 0.82664, angle RMSD: 83.222\n",
            "Epoch 0057 Batch 020/087: Loss: 12.177, dist RMSD: 8.7092, (length-normalized): 0.78536, angle RMSD: 86.702\n",
            "Epoch 0057 Batch 030/087: Loss: 11.296, dist RMSD: 7.7826, (length-normalized): 0.76459, angle RMSD: 87.844\n",
            "Epoch 0057 Batch 040/087: Loss: 13.335, dist RMSD: 9.7868, (length-normalized): 0.86174, angle RMSD: 88.714\n",
            "Epoch 0057 Batch 050/087: Loss: 12.357, dist RMSD: 8.9429, (length-normalized): 0.89483, angle RMSD: 85.343\n",
            "Epoch 0057 Batch 060/087: Loss: 12.134, dist RMSD: 8.8825, (length-normalized): 0.74063, angle RMSD: 81.277\n",
            "Epoch 0057 Batch 070/087: Loss: 11.775, dist RMSD: 8.3395, (length-normalized): 0.74997, angle RMSD: 85.884\n",
            "Epoch 0057 Batch 080/087: Loss: 13.099, dist RMSD: 9.8811, (length-normalized): 0.79824, angle RMSD: 80.455\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 057: Train Loss: 12.673, dist RMSD: 9.3055, (length-normalized): 0.81525, angle RMSD: 84.2 \n",
            "              Val Loss: 13.112, dist RMSD: 9.772, (length-normalized): 0.84206, angle RMSD: 83.497\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0058 Batch 000/087: Loss: 12.673, dist RMSD: 9.2263, (length-normalized): 0.81126, angle RMSD: 86.176\n",
            "Epoch 0058 Batch 010/087: Loss: 13.053, dist RMSD: 9.698, (length-normalized): 0.86727, angle RMSD: 83.862\n",
            "Epoch 0058 Batch 020/087: Loss: 12.518, dist RMSD: 9.1726, (length-normalized): 0.75875, angle RMSD: 83.647\n",
            "Epoch 0058 Batch 030/087: Loss: 12.136, dist RMSD: 8.8121, (length-normalized): 0.85224, angle RMSD: 83.086\n",
            "Epoch 0058 Batch 040/087: Loss: 11.946, dist RMSD: 8.5044, (length-normalized): 0.87271, angle RMSD: 86.051\n",
            "Epoch 0058 Batch 050/087: Loss: 12.61, dist RMSD: 9.2721, (length-normalized): 0.84232, angle RMSD: 83.435\n",
            "Epoch 0058 Batch 060/087: Loss: 12.331, dist RMSD: 9.0554, (length-normalized): 0.80296, angle RMSD: 81.898\n",
            "Epoch 0058 Batch 070/087: Loss: 13.319, dist RMSD: 9.9644, (length-normalized): 0.93923, angle RMSD: 83.856\n",
            "Epoch 0058 Batch 080/087: Loss: 13.082, dist RMSD: 9.7962, (length-normalized): 0.80062, angle RMSD: 82.152\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 058: Train Loss: 12.614, dist RMSD: 9.2633, (length-normalized): 0.8122, angle RMSD: 83.773 \n",
            "              Val Loss: 12.913, dist RMSD: 9.5997, (length-normalized): 0.8335, angle RMSD: 82.821\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0059 Batch 000/087: Loss: 12.861, dist RMSD: 9.4663, (length-normalized): 0.81017, angle RMSD: 84.868\n",
            "Epoch 0059 Batch 010/087: Loss: 13.1, dist RMSD: 9.5989, (length-normalized): 0.89446, angle RMSD: 87.523\n",
            "Epoch 0059 Batch 020/087: Loss: 11.348, dist RMSD: 8.0971, (length-normalized): 0.73319, angle RMSD: 81.285\n",
            "Epoch 0059 Batch 030/087: Loss: 13.457, dist RMSD: 10.294, (length-normalized): 0.84328, angle RMSD: 79.077\n",
            "Epoch 0059 Batch 040/087: Loss: 12.866, dist RMSD: 9.5432, (length-normalized): 0.82365, angle RMSD: 83.073\n",
            "Epoch 0059 Batch 050/087: Loss: 11.64, dist RMSD: 8.3931, (length-normalized): 0.74075, angle RMSD: 81.168\n",
            "Epoch 0059 Batch 060/087: Loss: 12.312, dist RMSD: 9.0991, (length-normalized): 0.742, angle RMSD: 80.328\n",
            "Epoch 0059 Batch 070/087: Loss: 11.953, dist RMSD: 8.666, (length-normalized): 0.7835, angle RMSD: 82.166\n",
            "Epoch 0059 Batch 080/087: Loss: 13.148, dist RMSD: 9.8415, (length-normalized): 0.85695, angle RMSD: 82.657\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 059: Train Loss: 12.578, dist RMSD: 9.2517, (length-normalized): 0.81064, angle RMSD: 83.166 \n",
            "              Val Loss: 12.987, dist RMSD: 9.6942, (length-normalized): 0.84135, angle RMSD: 82.32\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 0060 Batch 000/087: Loss: 12.182, dist RMSD: 8.8667, (length-normalized): 0.7646, angle RMSD: 82.891\n",
            "Epoch 0060 Batch 010/087: Loss: 12.248, dist RMSD: 8.712, (length-normalized): 0.80102, angle RMSD: 88.391\n",
            "Epoch 0060 Batch 020/087: Loss: 11.663, dist RMSD: 8.205, (length-normalized): 0.76251, angle RMSD: 86.446\n",
            "Epoch 0060 Batch 030/087: Loss: 11.463, dist RMSD: 8.0098, (length-normalized): 0.77565, angle RMSD: 86.333\n",
            "Epoch 0060 Batch 040/087: Loss: 12.542, dist RMSD: 9.2171, (length-normalized): 0.83398, angle RMSD: 83.113\n",
            "Epoch 0060 Batch 050/087: Loss: 12.69, dist RMSD: 9.4928, (length-normalized): 0.79016, angle RMSD: 79.941\n",
            "Epoch 0060 Batch 060/087: Loss: 12.913, dist RMSD: 9.4183, (length-normalized): 0.87179, angle RMSD: 87.364\n",
            "Epoch 0060 Batch 070/087: Loss: 12.628, dist RMSD: 9.2985, (length-normalized): 0.81602, angle RMSD: 83.248\n",
            "Epoch 0060 Batch 080/087: Loss: 12.536, dist RMSD: 9.3347, (length-normalized): 0.80003, angle RMSD: 80.029\n",
            "\n",
            "===========================================\n",
            "\n",
            "Epoch 060: Train Loss: 12.503, dist RMSD: 9.1985, (length-normalized): 0.80716, angle RMSD: 82.62 \n",
            "              Val Loss: 13.065, dist RMSD: 9.7991, (length-normalized): 0.85146, angle RMSD: 81.639\n",
            "\n",
            "===========================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWL3fuy5K0oF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys, time \n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.layers import *\n",
        "from model_utils import *\n",
        "from datetime import datetime\n",
        "\n",
        "#######################################\n",
        "# require tensorflow 2.0 \n",
        "# CPU-only: pip install tf-nightly-2.0-preview\n",
        "# GPU: pip install tf-nightly-2.0-gpu-preview\n",
        "print(tf.__version__)\n",
        "#######################################\n",
        "\n",
        "# python train_unet.py\n",
        "\n",
        "model_name = os.path.basename(sys.argv[0]).split(\".\")[0]   # or model name\n",
        "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-\" + model_name\n",
        "\n",
        "#######################################\n",
        "# Load data \n",
        "#######################################\n",
        "\n",
        "# replace this with your data directory\n",
        "data_dir = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/cu-deep-learning-spring19-hw2/\"\n",
        "\n",
        "folds_use = [1,2,3,4,5,6,7,8,9,10]\n",
        "data_fields = [[] for _ in range(9)]\n",
        "for i in folds_use:\n",
        "    with open(os.path.join(data_dir, 'train_fold_{}.pkl'.format(i)), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        for j in range(len(data_fields)):\n",
        "            data_fields[j].append(data[j])\n",
        "\n",
        "for j in range(len(data_fields)):\n",
        "    data_fields[j] = np.concatenate(data_fields[j])\n",
        "indices, pdbs, length_aas, pdb_aas, q8s, dcalphas, psis, phis, msas = data_fields\n",
        "print(\"Total number of protein sequences:\", len(pdb_aas))\n",
        "\n",
        "# filter out sequences that are too long \n",
        "maxlen_seq = 384\n",
        "minlen_seq = 10\n",
        "lenth_mask = (length_aas < maxlen_seq) & (length_aas > minlen_seq)\n",
        "for j in range(len(data_fields)):\n",
        "    data_fields[j] = data_fields[j][lenth_mask]\n",
        "indices, pdbs, length_aas, pdb_aas, q8s, dcalphas, psis, phis, msas = data_fields\n",
        "msas = [np.stack(x).transpose().astype(np.float32) for x in msas]\n",
        "phis = [np.array(x) for x in phis]\n",
        "psis = [np.array(x) for x in psis]\n",
        "print(\"Number of protein sequences shorter than {}: {}\".format(maxlen_seq, len(pdb_aas)))\n",
        "\n",
        "#############################################\n",
        "# Prepare dataset for traininng \n",
        "#############################################\n",
        "\n",
        "# Pad target distance matrix to the same size \n",
        "# Mask the padded regions later so that they don't contribute to the loss\n",
        "dcalphas_pad = np.zeros((len(dcalphas), maxlen_seq, maxlen_seq), dtype=np.float32)\n",
        "for i in range(len(dcalphas)):\n",
        "    length = dcalphas[i].shape[0]\n",
        "    dcalphas_pad[i, :length, :length] = dcalphas[i]\n",
        "\n",
        "# Pad target torsion angles to the same size \n",
        "train_target_phis = np.zeros([len(phis), maxlen_seq], dtype=np.float32)\n",
        "for i in range(len(phis)):\n",
        "    train_target_phis[i, :phis[i].shape[0]] = phis[i]\n",
        "train_target_psis = np.zeros([len(psis), maxlen_seq], dtype=np.float32)\n",
        "for i in range(len(psis)):\n",
        "    train_target_psis[i, :psis[i].shape[0]] = psis[i]\n",
        "\n",
        "# Input data \n",
        "train_input_seqs = pdb_aas\n",
        "train_input_grams = seq2ngrams(train_input_seqs, n=1)\n",
        "train_q8s = q8s\n",
        "train_q8s_grams = seq2ngrams(train_q8s, n=1)\n",
        "\n",
        "# Define tokenizer encoder the input sequence\n",
        "tokenizer_encoder = text.Tokenizer()\n",
        "tokenizer_encoder.fit_on_texts(train_input_grams)\n",
        "tokenizer_encoder_q8s = text.Tokenizer()\n",
        "tokenizer_encoder_q8s.fit_on_texts(train_q8s_grams)\n",
        "\n",
        "# Tokenize the input sequences for use in training\n",
        "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams)\n",
        "train_input_data = sequence.pad_sequences(train_input_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "train_q8s_data = tokenizer_encoder_q8s.texts_to_sequences(train_q8s_grams)\n",
        "train_q8s_data = sequence.pad_sequences(train_q8s_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "\n",
        "# The number of words and tags to be passed as parameters to the model\n",
        "n_words = len(tokenizer_encoder.word_index) + 1\n",
        "n_words_q8s = len(tokenizer_encoder_q8s.word_index) + 1\n",
        "\n",
        "# extra MSA features \n",
        "msa_dim = msas[0].shape[1]\n",
        "msas_padded = np.zeros([len(msas), maxlen_seq, msa_dim], dtype=np.float32)\n",
        "for i in range(len(msas)):\n",
        "    msas_padded[i, :msas[i].shape[0], :] = msas[i]\n",
        "\n",
        "# Train/validation set split \n",
        "training_idx = np.arange(2800)\n",
        "validation_idx = np.arange(2800, len(pdb_aas))\n",
        "\n",
        "X_train = train_input_data[training_idx]\n",
        "X_val = train_input_data[validation_idx]\n",
        "\n",
        "X_train_q8s = train_q8s_data[training_idx]\n",
        "X_val_q8s = train_q8s_data[validation_idx]\n",
        "\n",
        "X_train_msa = msas_padded[training_idx]\n",
        "X_val_msa = msas_padded[validation_idx]\n",
        "\n",
        "X_train_seqlen = length_aas[training_idx]\n",
        "X_val_seqlen = length_aas[validation_idx]\n",
        "\n",
        "y_train_dist_matrix = dcalphas_pad[training_idx]\n",
        "y_val_dist_matrix = dcalphas_pad[validation_idx]\n",
        "\n",
        "y_train_phis = train_target_phis[training_idx]\n",
        "y_val_phis = train_target_phis[validation_idx]\n",
        "y_train_psis = train_target_psis[training_idx]\n",
        "y_val_psis = train_target_psis[validation_idx]\n",
        "\n",
        "pdb_names_train = pdbs[training_idx]\n",
        "pdb_names_val = pdbs[validation_idx]\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Create a directory to save model checkpoints \n",
        "#############################################\n",
        "\n",
        "if not os.path.exists(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "log_dir = 'logs/{}'.format(model_name)\n",
        "os.mkdir(log_dir)\n",
        "\n",
        "with open(os.path.join(log_dir, 'tokenizer_encoder.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer_encoder, handle)\n",
        "\n",
        "with open(os.path.join(log_dir, 'tokenizer_encoder_q8s.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer_encoder_q8s, handle)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# build a neural network model\n",
        "#############################################\n",
        "\n",
        "# Here we use layer functions to build neural network graph (as opposed to defining a Model class)\n",
        "\n",
        "# Input layer: (None, ) shape allows the network to be flexible to input sequence length \n",
        "# inputs: [protein_sequence, q8_sequence, MSA features]\n",
        "inputs = [Input(shape = (None, )), Input(shape = (None, )), Input(shape = (None, 21))]\n",
        "\n",
        "# Word (amino acid) embedding layers \n",
        "embedding1 = Embedding(input_dim = n_words, output_dim = 128, input_length = None, input_shape=(None,))\n",
        "embedding2 = Embedding(input_dim = n_words_q8s, output_dim = 64, input_length = None, input_shape=(None,))\n",
        "embed_input = embedding1(inputs[0]) # raw seq\n",
        "embed_q8s = embedding2(inputs[1]) # q8\n",
        "\n",
        "\n",
        "merged_input = concatenate([embed_input, embed_q8s, inputs[2]], axis = 2)\n",
        "\n",
        "######### Raw Sequence data\n",
        "\n",
        "#### A seq-to-seq 1-D convolutional U-Net (https://arxiv.org/abs/1505.04597)\n",
        "# raw_seq = Conv_UNet(embed_input, droprate=0.1)\n",
        "# q8_seq =  Conv_UNet(embed_q8s, droprate=0.1)\n",
        "# msa_seq = Conv_UNet(inputs[2], droprate=0.1)\n",
        "conv1 = Conv_UNet(merged_input, droprate = 0.3)\n",
        "\n",
        "#### bidirectional RNN\n",
        "rnn = Bidirectional(GRU(units=32, return_sequences=True, recurrent_dropout=0.2, recurrent_activation='relu', bias_initializer= \"ones\"))\n",
        "GRU1 = rnn(merged_input)\n",
        "GRU1 = BatchNormalization()(conv1)\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Attention\n",
        "##################################################\n",
        "\n",
        "attention1 = Dense(1, activation='tanh')(GRU1)\n",
        "attention1 = Flatten()(attention1)\n",
        "attention1 = Activation('softmax')(attention1)\n",
        "attention1 = RepeatVector(rnn.output_shape[2]*2)(attention1)\n",
        "attention1 = Permute([2, 1])(attention1)\n",
        "\n",
        "\n",
        "attention_merge1 = multiply([attention1, GRU1])\n",
        "\n",
        "\n",
        "\n",
        "attention2 = Dense(1, activation='tanh')(conv1)\n",
        "attention2 = Flatten()(attention2)\n",
        "attention2 = Activation('softmax')(attention2)\n",
        "attention2 = RepeatVector(conv1.shape[2])(attention2)\n",
        "attention2 = Permute([2, 1])(attention2)\n",
        "\n",
        "\n",
        "attention_merge2 = multiply([attention2, conv1])\n",
        "\n",
        "#### A seq-to-seq 1-D convolutional U-Net (https://arxiv.org/abs/1505.04597)\n",
        "\n",
        "result = concatenate([attention_merge1, attention_merge2])\n",
        "\n",
        "#### Torsion angle prediction\n",
        "# The model output 3 angles (phi, psi, omega) for computing distance matrix. \n",
        "# But we only use phi, psi for evaluating angles since omega angle is usually fixed in the protein. \n",
        "#y = TimeDistributed(Dense(3, activation = \"tanh\"))(merged_out)\n",
        "#angles = tf.multiply(y, np.pi, name=\"torsion_angles\") \n",
        "\n",
        "# The RGN paper predict torsion angles using softmax probabilities over a learned alphabet/mixture of angles (https://www.biorxiv.org/content/biorxiv/early/2018/08/29/265231.full-text.pdf)\n",
        "y = TimeDistributed(Dense(50, activation = \"softmax\"))(result)\n",
        "angles = TorsionAngles(alphabet_size=50)(y)\n",
        "\n",
        "#### Build Model. \n",
        "# Define the computational graph using model input and model output\n",
        "model = tf.keras.Model(inputs, [angles, y])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "# Train the model\n",
        "##################################################\n",
        "\n",
        "batch_size = 32\n",
        "angle_scale = 180 / np.pi  # convert angles from [-pi, pi] to [-180, 180]\n",
        "n_iter = int(X_train.shape[0] / batch_size)\n",
        "\n",
        "#### For RNN (slower)\n",
        "# n_epochs = 8\n",
        "# lr_decay_iters = [n_iter * 3.0, n_iter * 6.0]  # lr decay at n_iters * epoch_steps\n",
        "# lr_steps = [0.001, 0.0005, 0.0001]  # learning rate decay steps\n",
        "\n",
        "#### For ConvNet\n",
        "n_epochs = 60\n",
        "lr_decay_iters = [n_iter * 5.0, n_iter * 8.0]  # lr decay at n_iters * epoch_steps\n",
        "lr_steps = [0.0001, 0.00005, 0.00002]  # learning rate decay steps\n",
        "\n",
        "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_decay_iters, lr_steps)\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate_fn)\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "torsion_loss_weight = 1.0 / 50\n",
        "dist_loss_weight = 1.0 \n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    \n",
        "    ################################### \n",
        "    # Train\n",
        "    ################################### \n",
        "    idx_shuffle = np.random.permutation(X_train.shape[0])\n",
        "\n",
        "    train_rmsd_dist = tf.metrics.Mean()\n",
        "    train_rmsd_dist_norm = tf.metrics.Mean()\n",
        "    train_rmsd_angle = tf.metrics.Mean()\n",
        "    train_rmsd_all = tf.metrics.Mean()\n",
        "\n",
        "    # Iterate through mini-batchs\n",
        "    for it in range(n_iter):\n",
        "        # Shuffle (alternatively, use tf.dataset)\n",
        "        idx_batch = idx_shuffle[it * batch_size : (it+1) * batch_size]\n",
        "        batch_seq = tf.convert_to_tensor(X_train[idx_batch])\n",
        "        batch_q8s = tf.convert_to_tensor(X_train_q8s[idx_batch])\n",
        "        batch_seqlen = tf.convert_to_tensor(X_train_seqlen[idx_batch], dtype=tf.float32)\n",
        "        batch_dcalphas = tf.convert_to_tensor(y_train_dist_matrix[idx_batch])\n",
        "        batch_msa = tf.convert_to_tensor(X_train_msa[idx_batch])\n",
        "\n",
        "        # Compute loss \n",
        "        with tf.GradientTape() as tape:\n",
        "            torsion_angles, y_out = model([batch_seq, batch_q8s, batch_msa])\n",
        "            phi, psi = torsion_angles[:, :, 0], torsion_angles[:, :, 1]  \n",
        "            phi_scaled, psi_scaled = phi * angle_scale, psi * angle_scale  # from [-pi, pi] to [-180, 180]\n",
        "            loss_phi_batch = rmsd_torsion_angle(phi_scaled, y_train_phis[idx_batch], batch_seqlen)\n",
        "            loss_psi_batch = rmsd_torsion_angle(psi_scaled, y_train_psis[idx_batch], batch_seqlen)\n",
        "            loss_phi = tf.reduce_mean(loss_phi_batch)  \n",
        "            loss_psi = tf.reduce_mean(loss_psi_batch)\n",
        "\n",
        "            dist_matrix, coordinates = DistanceMatrix()(torsion_angles)\n",
        "            loss_drmsd_batch = drmsd_dist_matrix(dist_matrix, batch_dcalphas, batch_seqlen)\n",
        "            loss_drmsd = tf.reduce_mean(loss_drmsd_batch)  # drmsd metric\n",
        "            loss_drmsd_normalized = tf.reduce_mean(loss_drmsd_batch / tf.sqrt(batch_seqlen))  # longer proteins have larger distance\n",
        "            \n",
        "            # the optimization objective can be 1. loss_drmsd or 2. (loss_phi + loss_psi), or both\n",
        "            # optimizing distance mattrix (drmsd) and tortion angles separately probably gives better rerults. \n",
        "            # loss_all = loss_drmsd   # distance matrix loss\n",
        "            # loss_all = loss_phi + loss_psi  # torsion angle loss\n",
        "            \n",
        "            loss_all = loss_drmsd * dist_loss_weight + (loss_phi + loss_psi) * torsion_loss_weight\n",
        "            \n",
        "\n",
        "        # Compute gradient \n",
        "        grads = tape.gradient(loss_all, model.trainable_variables)  # loss includ both distance matrix and torsion angles \n",
        "        # grads = tape.gradient(loss_all, model.trainable_variables)\n",
        "        grads, global_norm = tf.clip_by_global_norm(grads, 5.0)\n",
        "        \n",
        "        # Backprop\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\n",
        "\n",
        "        # Record metrics\n",
        "        train_rmsd_dist_norm(loss_drmsd_normalized)\n",
        "        train_rmsd_dist(loss_drmsd)\n",
        "        train_rmsd_angle((loss_phi + loss_psi)/2)\n",
        "        train_rmsd_all(loss_all)\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            print(\"Epoch {:04d} Batch {:03d}/{:03d}: Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g}\".format(\n",
        "                epoch, it, n_iter, loss_all, loss_drmsd, loss_drmsd_normalized, (loss_phi + loss_psi)/2))\n",
        "        \n",
        "    \n",
        "    ###################################  \n",
        "    # Validation \n",
        "    ################################### \n",
        "    idx_val = np.arange(X_val.shape[0])\n",
        "    n_iter_val = int(len(idx_val) / batch_size) + 1  # use all\n",
        "\n",
        "    val_rmsd_dist = tf.metrics.Mean()\n",
        "    val_rmsd_dist_norm = tf.metrics.Mean()\n",
        "    val_rmsd_angle = tf.metrics.Mean()\n",
        "    val_rmsd_all = tf.metrics.Mean()\n",
        "\n",
        "    for it in range(n_iter_val):\n",
        "        idx_batch = idx_val[it * batch_size : (it+1) * batch_size]\n",
        "        batch_seq = tf.convert_to_tensor(X_val[idx_batch])\n",
        "        batch_q8s = tf.convert_to_tensor(X_val_q8s[idx_batch])\n",
        "        batch_seqlen = tf.convert_to_tensor(X_val_seqlen[idx_batch], dtype=tf.float32)\n",
        "        batch_dcalphas = tf.convert_to_tensor(y_val_dist_matrix[idx_batch])\n",
        "        batch_msa = tf.convert_to_tensor(X_val_msa[idx_batch])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            torsion_angles, y_out = model([batch_seq, batch_q8s, batch_msa])\n",
        "            phi, psi = torsion_angles[:, :, 0], torsion_angles[:, :, 1]\n",
        "            phi_scaled, psi_scaled = phi * angle_scale, psi * angle_scale  # from [-pi, pi] to [-180, 180]\n",
        "            loss_phi_batch = rmsd_torsion_angle(phi_scaled, y_val_phis[idx_batch], batch_seqlen)\n",
        "            loss_psi_batch = rmsd_torsion_angle(psi_scaled, y_val_psis[idx_batch], batch_seqlen)\n",
        "            loss_phi = tf.reduce_mean(loss_phi_batch)  \n",
        "            loss_psi = tf.reduce_mean(loss_psi_batch)\n",
        "\n",
        "            # torsion_angles are on the scale of [-3.14, 3.14], the target labels are on the scale of [-180, 180]. Need to convert after prediction\n",
        "            dist_matrix, coordinates = DistanceMatrix()(torsion_angles)\n",
        "            loss_drmsd_batch = drmsd_dist_matrix(dist_matrix, batch_dcalphas, batch_seqlen)\n",
        "            loss_drmsd = tf.reduce_mean(loss_drmsd_batch)\n",
        "            loss_drmsd_normalized = tf.reduce_mean(loss_drmsd_batch / tf.sqrt(batch_seqlen))\n",
        "\n",
        "            #loss_all = loss_drmsd\n",
        "            # loss_all = loss_phi + loss_psi\n",
        "            loss_all = loss_drmsd * dist_loss_weight + (loss_phi + loss_psi) * torsion_loss_weight\n",
        "\n",
        "        # no gradient descent during validation\n",
        "        val_rmsd_dist_norm(loss_drmsd_normalized)\n",
        "        val_rmsd_dist(loss_drmsd)\n",
        "        val_rmsd_angle((loss_phi + loss_psi)/2)\n",
        "        val_rmsd_all(loss_all)\n",
        "\n",
        "        if it == 0:\n",
        "            plot_pred = dist_matrix[:8].numpy()\n",
        "            plot_gt = batch_dcalphas[:8].numpy()\n",
        "            plot_names = pdb_names_val[idx_batch][:8]\n",
        "            plot_len = batch_seqlen[:8].numpy().astype(int)\n",
        "            plot_rmds = loss_drmsd_batch[:8].numpy()\n",
        "            plot_dist_matrix(plot_pred, plot_gt, plot_names, plot_len, plot_rmds,\n",
        "                os.path.join(log_dir, \"plot_distance_matrix_epoch-{}.pdf\".format(epoch)))\n",
        "\n",
        "    ################################### \n",
        "\n",
        "    # plot history if not using tensorboard\n",
        "    train_loss_history.append([train_rmsd_all.result(), train_rmsd_dist.result(), train_rmsd_dist_norm.result(), train_rmsd_angle.result()])\n",
        "    val_loss_history.append([val_rmsd_all.result(), val_rmsd_dist.result(), val_rmsd_dist_norm.result(), val_rmsd_angle.result()])\n",
        "\n",
        "    print(\"\\n===========================================\\n\")\n",
        "    print(\"Epoch {:03d}: Train Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g} \\n \".format(\n",
        "        epoch, *train_loss_history[-1]) + \n",
        "        \"             Val Loss: {:.5g}, dist RMSD: {:.5g}, (length-normalized): {:.5g}, angle RMSD: {:.5g}\".format(\n",
        "        *val_loss_history[-1]))\n",
        "    print(\"\\n===========================================\\n\")\n",
        "\n",
        "    # save model checkpoints (this overwrites previous checkpoints)\n",
        "    tf.saved_model.save(model, log_dir)\n",
        "\n",
        "    # plot\n",
        "    train_loss_arr = np.array(train_loss_history).transpose()\n",
        "    val_loss_arr = np.array(val_loss_history).transpose()\n",
        "    plot_train_val(train_loss_arr[0], val_loss_arr[0], \n",
        "            title=\"RMSD (dist + angle) loss\", savepath=os.path.join(log_dir, \"rmsd_all.pdf\"))\n",
        "    plot_train_val(train_loss_arr[1], val_loss_arr[1], \n",
        "            title=\"RMSD (distance matrix) loss\", savepath=os.path.join(log_dir, \"rmsd_distance_matrix.pdf\"))\n",
        "    plot_train_val(train_loss_arr[2], val_loss_arr[2], \n",
        "            title=\"RMSD (distance matrix, lengh-normalized) loss\", savepath=os.path.join(log_dir, \"rmsd_distance_matrix_normalized.pdf\"))\n",
        "    plot_train_val(train_loss_arr[3], val_loss_arr[3], \n",
        "            title=\"RMSD (torsion angles) loss\", savepath=os.path.join(log_dir, \"rmsd_torsion_angles.pdf\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnIZXCJlY5Vf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare Output"
      ]
    },
    {
      "metadata": {
        "id": "q3eQMsALY45G",
        "colab_type": "code",
        "outputId": "c4b100dd-bef0-48a6-a490-f452bc95f5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os, sys, time \n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.layers import *\n",
        "from model_utils import *\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "########## load data and model\n",
        "data_dir = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/cu-deep-learning-spring19-hw2\"\n",
        "model_dir = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/logs/20190312-221750-ipykernel_launcher\"  # or your model dir\n",
        "\n",
        "data_fields = [[] for _ in range(6)]\n",
        "with open(os.path.join(data_dir, 'test.pkl'), 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    for j in range(len(data_fields)):\n",
        "        data_fields[j] = data[j]\n",
        "\n",
        "indices, pdbs, length_aas, pdb_aas, q8s, msas = data_fields\n",
        "msas = [np.stack(x).transpose().astype(np.float32) for x in msas]\n",
        "\n",
        "maxlen_seq = 384\n",
        "\n",
        "# Input data \n",
        "test_input_seqs = pdb_aas\n",
        "test_input_grams = seq2ngrams(test_input_seqs, n=1)\n",
        "test_q8s = q8s\n",
        "test_q8s_grams = seq2ngrams(test_q8s, n=1)\n",
        "\n",
        "# Tokenizer encoder for the input sequence\n",
        "with open(os.path.join(model_dir, 'tokenizer_encoder.pickle'), 'rb') as handle:\n",
        "    tokenizer_encoder = pickle.load(handle)\n",
        "with open(os.path.join(model_dir, 'tokenizer_encoder_q8s.pickle'), 'rb') as handle:\n",
        "    tokenizer_encoder_q8s = pickle.load(handle)\n",
        "\n",
        "# Tokenize the input sequences for use in testing\n",
        "test_input_data = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
        "test_input_data = sequence.pad_sequences(test_input_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "test_q8s_data = tokenizer_encoder_q8s.texts_to_sequences(test_q8s_grams)\n",
        "test_q8s_data = sequence.pad_sequences(test_q8s_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
        "\n",
        "# extra MSA features \n",
        "msa_dim = msas[0].shape[1]\n",
        "msas_padded = np.zeros([len(msas), maxlen_seq, msa_dim], dtype=np.float32)\n",
        "for i in range(len(msas)):\n",
        "    msas_padded[i, :msas[i].shape[0], :] = msas[i]\n",
        "\n",
        "X_test = tf.convert_to_tensor(test_input_data, dtype=tf.float32)\n",
        "X_test_q8s = tf.convert_to_tensor(test_q8s_data, dtype=tf.float32)\n",
        "X_test_msa = tf.convert_to_tensor(msas_padded, dtype=tf.float32)\n",
        "X_test_seqlen = length_aas\n",
        "\n",
        "\n",
        "\n",
        "# prediction\n",
        "angle_scale = 180 / np.pi \n",
        "\n",
        "imported = tf.saved_model.load(model_dir)\n",
        "# loading a model from tf.saved_model.save() gives a forward pass function\n",
        "model_func = imported.signatures['serving_default']\n",
        "model_out = model_func(input_7=X_test, input_8=X_test_q8s, input_9=X_test_msa)\n",
        "torsion_angles = model_out['torsion_angles_2']\n",
        "\n",
        "phi, psi = torsion_angles[:, :, 0], torsion_angles[:, :, 1]  \n",
        "phi_scaled, psi_scaled = phi * angle_scale, psi * angle_scale  # from [-pi, pi] to [-180, 180]\n",
        "dist_matrix, coordinates = DistanceMatrix()(torsion_angles)\n",
        "\n",
        "phi_scaled, psi_scaled = phi_scaled.numpy(), psi_scaled.numpy()\n",
        "dist_matrix = dist_matrix.numpy()\n",
        "\n",
        "phi_output, psi_output, dist_matrix_output = [], [], []\n",
        "for i in range(X_test.shape[0]):\n",
        "    phi_output.append(phi_scaled[i, :X_test_seqlen[i]])\n",
        "    psi_output.append(psi_scaled[i, :X_test_seqlen[i]])\n",
        "    dist_matrix_output.append(dist_matrix[i, :X_test_seqlen[i], :X_test_seqlen[i]])\n",
        "\n",
        "output = [dist_matrix_output, psi_output, phi_output]\n",
        "with open(os.path.join(model_dir, 'predictions.pkl'), 'wb') as handle:\n",
        "    pickle.dump(output, handle)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-dev20190227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WN8e7Ri2putH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle, sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# load data\n",
        "model_dir = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/logs/20190312-221750-ipykernel_launcher\"  # or your model dir \n",
        "datafile = model_dir +\"/predictions.pkl\"\n",
        "with open(datafile, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "dist, psi, phi = data\n",
        "\n",
        "# load protein names\n",
        "testfile = \"/gdrive/My Drive/Colab Notebooks/DL-HW2/cu-deep-learning-spring19-hw2/test.csv\"\n",
        "test_input = pd.read_csv(testfile, header=None)\n",
        "protein_names = np.array(test_input.iloc[:,1])\n",
        "protein_len = np.array(test_input.iloc[:,2])\n",
        "\n",
        "# concatenate all output to one-dimensional\n",
        "all_data = []\n",
        "all_names = []\n",
        "for i, pname in enumerate(protein_names):\n",
        "    dist_flat = dist[i].ravel()\n",
        "    array = np.concatenate([dist_flat, psi[i], phi[i]])\n",
        "    all_data.append(array)\n",
        "\n",
        "    length = protein_len[i]\n",
        "    dist_names = [\"{}_d_{}_{}\".format(pname, i + 1, j + 1) for i in range(length) for\n",
        "            j in range(length)]\n",
        "\n",
        "    psi_names = [\"{}_psi_{}\".format(pname, i + 1) for i in range(length)]\n",
        "    phi_names = [\"{}_phi_{}\".format(pname, i + 1) for i in range(length)]\n",
        "    row_names = np.array(dist_names + psi_names + phi_names)\n",
        "    all_names.append(row_names)\n",
        "\n",
        "all_data = np.concatenate(all_data)\n",
        "all_names = np.concatenate(all_names)\n",
        "output = {\"Id\": all_names, \"Predicted\": all_data}\n",
        "output = pd.DataFrame(output)\n",
        "output.to_csv(os.path.join(model_dir, \"submission.csv\"), index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}